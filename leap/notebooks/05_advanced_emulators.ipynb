{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Climate Emulators: 1D CNN & Spherical FNO\n",
        "\n",
        "This notebook implements advanced architectures for climate emulation:\n",
        "\n",
        "1. **1D CNN (Vertical Profile)** - Convolutional layers along the vertical dimension\n",
        "2. **Stochastic Output Head** - Uncertainty quantification via parameterized Gaussian\n",
        "3. **Per-Variable-Group R² Monitoring** - Separate metrics for temperature, moisture, clouds\n",
        "4. **Multi-GPU Training** - JAX pmap/shmap for data parallelism\n",
        "5. **Spherical FNO** - Reference implementation (advanced)\n",
        "\n",
        "## Why 1D CNN?\n",
        "\n",
        "- **Vertical Structure**: Atmospheric columns have clear vertical structure\n",
        "- **Local Patterns**: Physics often involves local vertical interactions\n",
        "- **Inductive Bias**: CNNs naturally capture spatial hierarchies\n",
        "- **Parameter Efficiency**: Fewer parameters than fully-connected layers\n",
        "\n",
        "## Why Stochastic Head?\n",
        "\n",
        "- **Uncertainty**: Real atmosphere is stochastic\n",
        "- **Ensemble Emulation**: Capture distribution of possible outcomes\n",
        "- **Risk Assessment**: Important for extreme events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit, vmap, pmap\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import Any, Callable, Sequence, Optional, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass, field\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "# Try to import torch-harmonics (for SFNO)\n",
        "try:\n",
        "    import torch\n",
        "    import torch_harmonics as th\n",
        "    from torch_harmonics import RealSHT, InverseRealSHT\n",
        "    TORCH_HARMONICS_AVAILABLE = True\n",
        "    print(f\"✓ torch-harmonics available (version: {th.__version__ if hasattr(th, '__version__') else 'unknown'})\")\n",
        "except ImportError:\n",
        "    TORCH_HARMONICS_AVAILABLE = False\n",
        "    print(\"⚠ torch-harmonics not available. SFNO will not be functional.\")\n",
        "    print(\"  Install with: pip install torch-harmonics\")\n",
        "\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAX devices: {jax.devices()}\")\n",
        "print(f\"Number of devices: {jax.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # Data paths\n",
        "    DATA_DIR: Path = Path(\"/home/jovyan/leap-scratch\") / os.environ.get(\"USER\", \"user\") / \"data\"\n",
        "    MODEL_DIR: Path = Path(\"/home/jovyan/leap-scratch\") / os.environ.get(\"USER\", \"user\") / \"models\" / \"advanced\"\n",
        "    \n",
        "    # Model architecture\n",
        "    MODEL_TYPE: str = \"cnn1d\"  # \"cnn1d\", \"sfno\" - change to \"sfno\" to use Spherical FNO\n",
        "    \n",
        "    # 1D CNN architecture\n",
        "    CNN_CHANNELS: Sequence[int] = (64, 128, 256, 128, 64)  # Channel progression\n",
        "    CNN_KERNEL_SIZE: int = 3  # Kernel size for conv layers\n",
        "    USE_RESIDUAL: bool = True  # Use residual connections\n",
        "    USE_BATCH_NORM: bool = False  # Use batch norm (GroupNorm better for small batches)\n",
        "    USE_GROUP_NORM: bool = True  # Use group norm\n",
        "    NUM_GROUPS: int = 8  # Number of groups for GroupNorm\n",
        "    \n",
        "    # Stochastic output\n",
        "    STOCHASTIC_OUTPUT: bool = True  # Enable uncertainty quantification\n",
        "    MIN_STD: float = 1e-4  # Minimum std to prevent collapse\n",
        "    \n",
        "    # SFNO architecture (if MODEL_TYPE == \"sfno\")\n",
        "    SFNO_MODES: int = 16  # Number of Fourier modes\n",
        "    SFNO_WIDTH: int = 256  # Hidden dimension\n",
        "    SFNO_LAYERS: int = 4  # Number of FNO layers\n",
        "    \n",
        "    # Training\n",
        "    BATCH_SIZE: int = 128  # Per-device batch size\n",
        "    NUM_EPOCHS: int = 50\n",
        "    LEARNING_RATE: float = 1e-3\n",
        "    WEIGHT_DECAY: float = 1e-5\n",
        "    WARMUP_EPOCHS: int = 5\n",
        "    DROPOUT_RATE: float = 0.1\n",
        "    \n",
        "    # Loss weights\n",
        "    MSE_WEIGHT: float = 1.0\n",
        "    NLL_WEIGHT: float = 0.1  # Negative log-likelihood for stochastic output\n",
        "    WATER_CONS_WEIGHT: float = 0.0  # Optional water conservation\n",
        "    \n",
        "    # Multi-GPU\n",
        "    USE_PMAP: bool = jax.device_count() > 1\n",
        "    \n",
        "    # Variable groups for monitoring\n",
        "    VAR_GROUPS: dict = field(default_factory=lambda: {\n",
        "        'temperature': ['ptend_t'],\n",
        "        'moisture': ['ptend_q0001', 'ptend_q0002', 'ptend_q0003'],\n",
        "        'clouds': ['ptend_u', 'ptend_v'],  # Adjust based on actual variables\n",
        "    })\n",
        "    \n",
        "    # Checkpoint\n",
        "    SAVE_EVERY: int = 5\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        self.DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        self.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "config = Config()\n",
        "print(f\"Config: {config.MODEL_TYPE.upper()} architecture\")\n",
        "print(f\"Multi-GPU: {config.USE_PMAP} ({jax.device_count()} devices)\")\n",
        "print(f\"Stochastic output: {config.STOCHASTIC_OUTPUT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Preprocessed Data\n",
        "\n",
        "We'll load the preprocessed data from the previous notebook. For 1D CNN, we need to reshape the data to include a spatial dimension (vertical levels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data\n",
        "data_path = config.DATA_DIR / \"climsim_nyc_processed.npz\"\n",
        "\n",
        "if data_path.exists():\n",
        "    print(f\"Loading data from {data_path}\")\n",
        "    data = np.load(data_path)\n",
        "    X_train = data['X_train']\n",
        "    y_train = data['y_train']\n",
        "    X_val = data['X_val']\n",
        "    y_val = data['y_val']\n",
        "    X_test = data['X_test']\n",
        "    y_test = data['y_test']\n",
        "    input_mean = data['input_mean']\n",
        "    input_std = data['input_std']\n",
        "    output_mean = data['output_mean']\n",
        "    output_std = data['output_std']\n",
        "    print(f\"✓ Loaded {X_train.shape[0]} training samples\")\n",
        "else:\n",
        "    print(\"⚠ Preprocessed data not found. Generating synthetic data for demonstration...\")\n",
        "    # Generate synthetic data\n",
        "    n_train, n_val, n_test = 8000, 1000, 1000\n",
        "    n_levels = 60  # Typical vertical levels\n",
        "    n_vars = 5  # Variables per level (T, q, u, v, etc.)\n",
        "    input_dim = n_levels * n_vars  # ~300\n",
        "    output_dim = n_levels * 3  # Temperature, moisture, momentum tendencies\n",
        "    \n",
        "    X_train = np.random.randn(n_train, input_dim).astype(np.float32)\n",
        "    y_train = np.random.randn(n_train, output_dim).astype(np.float32) * 0.1\n",
        "    X_val = np.random.randn(n_val, input_dim).astype(np.float32)\n",
        "    y_val = np.random.randn(n_val, output_dim).astype(np.float32) * 0.1\n",
        "    X_test = np.random.randn(n_test, input_dim).astype(np.float32)\n",
        "    y_test = np.random.randn(n_test, output_dim).astype(np.float32) * 0.1\n",
        "    \n",
        "    input_mean = np.zeros(input_dim, dtype=np.float32)\n",
        "    input_std = np.ones(input_dim, dtype=np.float32)\n",
        "    output_mean = np.zeros(output_dim, dtype=np.float32)\n",
        "    output_std = np.ones(output_dim, dtype=np.float32)\n",
        "    print(f\"✓ Generated synthetic data\")\n",
        "\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"  Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Val:   X={X_val.shape}, y={y_val.shape}\")\n",
        "print(f\"  Test:  X={X_test.shape}, y={y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reshape Data for 1D CNN\n",
        "\n",
        "CNNs expect spatial dimensions. We'll reshape flattened vectors into (levels, channels) format:\n",
        "- Input: (batch, features) → (batch, levels, channels)\n",
        "- Output: (batch, features) → (batch, levels, output_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Infer structure from data dimensions\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "# Common vertical levels in climate models\n",
        "possible_levels = [26, 30, 60, 72, 91]\n",
        "n_levels = None\n",
        "\n",
        "for levels in possible_levels:\n",
        "    if input_dim % levels == 0:\n",
        "        n_levels = levels\n",
        "        n_input_vars = input_dim // levels\n",
        "        break\n",
        "\n",
        "if n_levels is None:\n",
        "    # Default: assume 60 levels\n",
        "    n_levels = 60\n",
        "    n_input_vars = input_dim // n_levels\n",
        "    if input_dim % n_levels != 0:\n",
        "        # Pad to make it divisible\n",
        "        pad_size = n_levels - (input_dim % n_levels)\n",
        "        X_train = np.pad(X_train, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        X_val = np.pad(X_val, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        X_test = np.pad(X_test, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        input_dim = X_train.shape[1]\n",
        "        n_input_vars = input_dim // n_levels\n",
        "\n",
        "# Same for output\n",
        "if output_dim % n_levels == 0:\n",
        "    n_output_vars = output_dim // n_levels\n",
        "else:\n",
        "    pad_size = n_levels - (output_dim % n_levels)\n",
        "    y_train = np.pad(y_train, ((0, 0), (0, pad_size)), mode='constant')\n",
        "    y_val = np.pad(y_val, ((0, 0), (0, pad_size)), mode='constant')\n",
        "    y_test = np.pad(y_test, ((0, 0), (0, pad_size)), mode='constant')\n",
        "    output_dim = y_train.shape[1]\n",
        "    n_output_vars = output_dim // n_levels\n",
        "\n",
        "print(f\"\\nInferred structure:\")\n",
        "print(f\"  Vertical levels: {n_levels}\")\n",
        "print(f\"  Input variables per level: {n_input_vars}\")\n",
        "print(f\"  Output variables per level: {n_output_vars}\")\n",
        "\n",
        "# Reshape: (batch, features) -> (batch, levels, vars_per_level)\n",
        "def reshape_for_cnn(X, n_levels, n_vars):\n",
        "    \"\"\"Reshape flattened data to (batch, levels, channels)\"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    return X.reshape(batch_size, n_levels, n_vars)\n",
        "\n",
        "X_train_cnn = reshape_for_cnn(X_train, n_levels, n_input_vars)\n",
        "X_val_cnn = reshape_for_cnn(X_val, n_levels, n_input_vars)\n",
        "X_test_cnn = reshape_for_cnn(X_test, n_levels, n_input_vars)\n",
        "\n",
        "y_train_cnn = reshape_for_cnn(y_train, n_levels, n_output_vars)\n",
        "y_val_cnn = reshape_for_cnn(y_val, n_levels, n_output_vars)\n",
        "y_test_cnn = reshape_for_cnn(y_test, n_levels, n_output_vars)\n",
        "\n",
        "print(f\"\\nReshaped for CNN:\")\n",
        "print(f\"  X_train: {X_train_cnn.shape} (batch, levels, input_channels)\")\n",
        "print(f\"  y_train: {y_train_cnn.shape} (batch, levels, output_channels)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1D CNN Architecture\n",
        "\n",
        "We'll build a 1D CNN with:\n",
        "- Residual connections (inspired by ResNet)\n",
        "- Group normalization (better than batch norm for small batches)\n",
        "- Stochastic output head (mean + log_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock1D(nn.Module):\n",
        "    \"\"\"1D Residual block with optional normalization\"\"\"\n",
        "    channels: int\n",
        "    kernel_size: int = 3\n",
        "    use_group_norm: bool = True\n",
        "    num_groups: int = 8\n",
        "    dropout_rate: float = 0.1\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        residual = x\n",
        "        \n",
        "        # First conv\n",
        "        x = nn.Conv(self.channels, kernel_size=(self.kernel_size,), padding='SAME')(x)\n",
        "        if self.use_group_norm:\n",
        "            x = nn.GroupNorm(num_groups=self.num_groups)(x)\n",
        "        x = nn.swish(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
        "        \n",
        "        # Second conv\n",
        "        x = nn.Conv(self.channels, kernel_size=(self.kernel_size,), padding='SAME')(x)\n",
        "        if self.use_group_norm:\n",
        "            x = nn.GroupNorm(num_groups=self.num_groups)(x)\n",
        "        \n",
        "        # Residual connection (with projection if needed)\n",
        "        if residual.shape[-1] != self.channels:\n",
        "            residual = nn.Conv(self.channels, kernel_size=(1,), padding='SAME')(residual)\n",
        "        \n",
        "        x = nn.swish(x + residual)\n",
        "        return x\n",
        "\n",
        "\n",
        "class StochasticOutputHead(nn.Module):\n",
        "    \"\"\"Output head that predicts mean and log_std for uncertainty quantification\"\"\"\n",
        "    output_dim: int\n",
        "    min_std: float = 1e-4\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # Predict mean\n",
        "        mean = nn.Dense(self.output_dim, name='mean')(x)\n",
        "        \n",
        "        # Predict log(std) - using log ensures std > 0\n",
        "        log_std = nn.Dense(self.output_dim, name='log_std')(x)\n",
        "        # Clamp to prevent numerical issues\n",
        "        log_std = jnp.clip(log_std, jnp.log(self.min_std), 5.0)\n",
        "        \n",
        "        return mean, log_std\n",
        "\n",
        "\n",
        "class CNN1DClimateEmulator(nn.Module):\n",
        "    \"\"\"1D CNN Climate Emulator with optional stochastic output\"\"\"\n",
        "    channels: Sequence[int] = (64, 128, 256, 128, 64)\n",
        "    output_channels: int = 3  # Number of output variables per level\n",
        "    kernel_size: int = 3\n",
        "    use_residual: bool = True\n",
        "    use_group_norm: bool = True\n",
        "    num_groups: int = 8\n",
        "    dropout_rate: float = 0.1\n",
        "    stochastic_output: bool = True\n",
        "    min_std: float = 1e-4\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        # x shape: (batch, levels, input_channels)\n",
        "        \n",
        "        # Initial projection\n",
        "        x = nn.Conv(self.channels[0], kernel_size=(1,), padding='SAME')(x)\n",
        "        \n",
        "        # Residual blocks with increasing then decreasing channels\n",
        "        if self.use_residual:\n",
        "            for ch in self.channels:\n",
        "                x = ResidualBlock1D(\n",
        "                    channels=ch,\n",
        "                    kernel_size=self.kernel_size,\n",
        "                    use_group_norm=self.use_group_norm,\n",
        "                    num_groups=self.num_groups,\n",
        "                    dropout_rate=self.dropout_rate\n",
        "                )(x, training=training)\n",
        "        else:\n",
        "            # Simple conv layers without residual\n",
        "            for ch in self.channels:\n",
        "                x = nn.Conv(ch, kernel_size=(self.kernel_size,), padding='SAME')(x)\n",
        "                if self.use_group_norm:\n",
        "                    x = nn.GroupNorm(num_groups=self.num_groups)(x)\n",
        "                x = nn.swish(x)\n",
        "                x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
        "        \n",
        "        # Output projection: (batch, levels, channels) -> (batch, levels, output_channels)\n",
        "        if self.stochastic_output:\n",
        "            # Flatten to apply dense layers\n",
        "            batch, levels, features = x.shape\n",
        "            x_flat = x.reshape(batch * levels, features)\n",
        "            \n",
        "            # Stochastic head\n",
        "            mean, log_std = StochasticOutputHead(\n",
        "                output_dim=self.output_channels,\n",
        "                min_std=self.min_std\n",
        "            )(x_flat)\n",
        "            \n",
        "            # Reshape back\n",
        "            mean = mean.reshape(batch, levels, self.output_channels)\n",
        "            log_std = log_std.reshape(batch, levels, self.output_channels)\n",
        "            \n",
        "            return mean, log_std\n",
        "        else:\n",
        "            # Deterministic output\n",
        "            x = nn.Conv(self.output_channels, kernel_size=(1,), padding='SAME')(x)\n",
        "            return x, None\n",
        "\n",
        "\n",
        "print(\"✓ 1D CNN architecture defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = CNN1DClimateEmulator(\n",
        "    channels=config.CNN_CHANNELS,\n",
        "    output_channels=n_output_vars,\n",
        "    kernel_size=config.CNN_KERNEL_SIZE,\n",
        "    use_residual=config.USE_RESIDUAL,\n",
        "    use_group_norm=config.USE_GROUP_NORM,\n",
        "    num_groups=config.NUM_GROUPS,\n",
        "    dropout_rate=config.DROPOUT_RATE,\n",
        "    stochastic_output=config.STOCHASTIC_OUTPUT,\n",
        "    min_std=config.MIN_STD\n",
        ")\n",
        "\n",
        "# Initialize parameters\n",
        "rng = random.PRNGKey(42)\n",
        "rng, init_rng, dropout_rng = random.split(rng, 3)\n",
        "\n",
        "# Sample input\n",
        "sample_input = jnp.ones((1, n_levels, n_input_vars))\n",
        "variables = model.init({'params': init_rng, 'dropout': dropout_rng}, sample_input, training=False)\n",
        "params = variables['params']\n",
        "\n",
        "# Count parameters\n",
        "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Model: 1D CNN Climate Emulator\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Parameters: {param_count:,}\")\n",
        "print(f\"Architecture:\")\n",
        "print(f\"  - Input: {n_levels} levels × {n_input_vars} variables\")\n",
        "print(f\"  - Channels: {config.CNN_CHANNELS}\")\n",
        "print(f\"  - Kernel size: {config.CNN_KERNEL_SIZE}\")\n",
        "print(f\"  - Residual: {config.USE_RESIDUAL}\")\n",
        "print(f\"  - Stochastic output: {config.STOCHASTIC_OUTPUT}\")\n",
        "print(f\"  - Output: {n_levels} levels × {n_output_vars} tendencies\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Test forward pass\n",
        "mean, log_std = model.apply(variables, sample_input, training=False, rngs={'dropout': dropout_rng})\n",
        "print(f\"\\nTest forward pass:\")\n",
        "print(f\"  Mean shape: {mean.shape}\")\n",
        "if log_std is not None:\n",
        "    print(f\"  Log-std shape: {log_std.shape}\")\n",
        "    print(f\"  Std range: [{jnp.exp(log_std).min():.6f}, {jnp.exp(log_std).max():.6f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions with Uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(pred, target):\n",
        "    \"\"\"Mean Squared Error\"\"\"\n",
        "    return jnp.mean((pred - target) ** 2)\n",
        "\n",
        "def negative_log_likelihood(mean, log_std, target):\n",
        "    \"\"\"Negative log-likelihood for Gaussian distribution\n",
        "    \n",
        "    NLL = 0.5 * log(2π) + log_std + 0.5 * ((target - mean) / std)^2\n",
        "    \"\"\"\n",
        "    std = jnp.exp(log_std)\n",
        "    nll = 0.5 * jnp.log(2 * jnp.pi) + log_std + 0.5 * ((target - mean) / std) ** 2\n",
        "    return jnp.mean(nll)\n",
        "\n",
        "def water_conservation_loss(pred):\n",
        "    \"\"\"Water conservation constraint (optional)\"\"\"\n",
        "    # Sum of moisture tendencies should be small\n",
        "    # This is a simplified version - adjust based on actual physics\n",
        "    moisture_sum = jnp.sum(pred, axis=-1)  # Sum over output variables\n",
        "    return jnp.mean(moisture_sum ** 2)\n",
        "\n",
        "def compute_loss(params, variables, batch_x, batch_y, rng, training=True):\n",
        "    \"\"\"Combined loss with MSE and optional NLL for stochastic output\"\"\"\n",
        "    mean, log_std = model.apply(variables, batch_x, training=training, rngs={'dropout': rng})\n",
        "    \n",
        "    # MSE loss on mean prediction\n",
        "    mse = mse_loss(mean, batch_y)\n",
        "    loss = config.MSE_WEIGHT * mse\n",
        "    \n",
        "    # NLL loss for uncertainty\n",
        "    if config.STOCHASTIC_OUTPUT and log_std is not None:\n",
        "        nll = negative_log_likelihood(mean, log_std, batch_y)\n",
        "        loss = loss + config.NLL_WEIGHT * nll\n",
        "    else:\n",
        "        nll = 0.0\n",
        "    \n",
        "    # Optional water conservation\n",
        "    if config.WATER_CONS_WEIGHT > 0:\n",
        "        water_cons = water_conservation_loss(mean)\n",
        "        loss = loss + config.WATER_CONS_WEIGHT * water_cons\n",
        "    else:\n",
        "        water_cons = 0.0\n",
        "    \n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'mse': mse,\n",
        "        'nll': nll,\n",
        "        'water_cons': water_cons\n",
        "    }\n",
        "    \n",
        "    return loss, (mean, log_std, metrics)\n",
        "\n",
        "print(\"✓ Loss functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-Variable-Group R² Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_r2_score(pred, target):\n",
        "    \"\"\"Compute R² score\"\"\"\n",
        "    ss_res = jnp.sum((target - pred) ** 2)\n",
        "    ss_tot = jnp.sum((target - jnp.mean(target)) ** 2)\n",
        "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
        "    return r2\n",
        "\n",
        "def compute_per_variable_r2(pred, target, var_groups, output_dim):\n",
        "    \"\"\"Compute R² for each variable group\n",
        "    \n",
        "    Note: This is a simplified version. In practice, you'd map variable names\n",
        "    to output dimensions based on your data structure.\n",
        "    \"\"\"\n",
        "    # Flatten spatial dimensions for per-variable metrics\n",
        "    pred_flat = pred.reshape(-1, output_dim)\n",
        "    target_flat = target.reshape(-1, output_dim)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # For demonstration, assume variables are ordered:\n",
        "    # [temp_vars, moisture_vars, cloud_vars]\n",
        "    # Adjust based on actual data structure\n",
        "    \n",
        "    # Example: first 1/3 are temperature, second 1/3 moisture, last 1/3 clouds\n",
        "    n_per_group = output_dim // 3\n",
        "    \n",
        "    groups = {\n",
        "        'temperature': (0, n_per_group),\n",
        "        'moisture': (n_per_group, 2 * n_per_group),\n",
        "        'clouds': (2 * n_per_group, output_dim)\n",
        "    }\n",
        "    \n",
        "    for group_name, (start, end) in groups.items():\n",
        "        if end > start:\n",
        "            pred_group = pred_flat[:, start:end]\n",
        "            target_group = target_flat[:, start:end]\n",
        "            r2 = compute_r2_score(pred_group, target_group)\n",
        "            results[f'r2_{group_name}'] = r2\n",
        "    \n",
        "    # Overall R²\n",
        "    results['r2_overall'] = compute_r2_score(pred_flat, target_flat)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Per-variable R² metrics defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Learning rate schedule\n",
        "steps_per_epoch = len(X_train_cnn) // config.BATCH_SIZE\n",
        "total_steps = steps_per_epoch * config.NUM_EPOCHS\n",
        "warmup_steps = steps_per_epoch * config.WARMUP_EPOCHS\n",
        "\n",
        "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=1e-8,\n",
        "    peak_value=config.LEARNING_RATE,\n",
        "    warmup_steps=warmup_steps,\n",
        "    decay_steps=total_steps,\n",
        "    end_value=1e-6\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=lr_schedule,\n",
        "    weight_decay=config.WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Train state\n",
        "class TrainState(train_state.TrainState):\n",
        "    dropout_rng: jax.random.PRNGKey\n",
        "\n",
        "state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=optimizer,\n",
        "    dropout_rng=dropout_rng\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining setup:\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n",
        "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training & Evaluation Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training step (JIT compiled)\n",
        "@jit\n",
        "def train_step(state, batch_x, batch_y):\n",
        "    \"\"\"Single training step\"\"\"\n",
        "    dropout_rng, new_dropout_rng = random.split(state.dropout_rng)\n",
        "    \n",
        "    def loss_fn(params):\n",
        "        variables = {'params': params}\n",
        "        return compute_loss(params, variables, batch_x, batch_y, dropout_rng, training=True)\n",
        "    \n",
        "    (loss, (mean, log_std, metrics)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    state = state.replace(dropout_rng=new_dropout_rng)\n",
        "    \n",
        "    return state, metrics\n",
        "\n",
        "# Evaluation step (JIT compiled)\n",
        "@jit\n",
        "def eval_step(state, batch_x, batch_y):\n",
        "    \"\"\"Single evaluation step\"\"\"\n",
        "    variables = {'params': state.params}\n",
        "    loss, (mean, log_std, metrics) = compute_loss(\n",
        "        state.params, variables, batch_x, batch_y, state.dropout_rng, training=False\n",
        "    )\n",
        "    return mean, log_std, metrics\n",
        "\n",
        "print(\"✓ Training and evaluation steps defined (JIT compiled)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-GPU Data Parallel Training (Optional)\n",
        "\n",
        "If multiple GPUs are available, we'll use `pmap` for data parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config.USE_PMAP:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Multi-GPU Training Setup\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Devices: {jax.device_count()}\")\n",
        "    \n",
        "    # Replicate state across devices\n",
        "    state = jax.device_put_replicated(state, jax.devices())\n",
        "    \n",
        "    # Define pmapped training step\n",
        "    @partial(pmap, axis_name='batch')\n",
        "    def train_step_pmap(state, batch_x, batch_y):\n",
        "        \"\"\"Pmapped training step\"\"\"\n",
        "        dropout_rng, new_dropout_rng = random.split(state.dropout_rng)\n",
        "        \n",
        "        def loss_fn(params):\n",
        "            variables = {'params': params}\n",
        "            return compute_loss(params, variables, batch_x, batch_y, dropout_rng, training=True)\n",
        "        \n",
        "        (loss, (mean, log_std, metrics)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "        \n",
        "        # Average gradients across devices\n",
        "        grads = jax.lax.pmean(grads, axis_name='batch')\n",
        "        \n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        state = state.replace(dropout_rng=new_dropout_rng)\n",
        "        \n",
        "        # Average metrics across devices\n",
        "        metrics = jax.tree_map(lambda x: jax.lax.pmean(x, axis_name='batch'), metrics)\n",
        "        \n",
        "        return state, metrics\n",
        "    \n",
        "    @partial(pmap, axis_name='batch')\n",
        "    def eval_step_pmap(state, batch_x, batch_y):\n",
        "        \"\"\"Pmapped evaluation step\"\"\"\n",
        "        variables = {'params': state.params}\n",
        "        loss, (mean, log_std, metrics) = compute_loss(\n",
        "            state.params, variables, batch_x, batch_y, state.dropout_rng, training=False\n",
        "        )\n",
        "        metrics = jax.tree_map(lambda x: jax.lax.pmean(x, axis_name='batch'), metrics)\n",
        "        return mean, log_std, metrics\n",
        "    \n",
        "    # Use pmap versions\n",
        "    train_step_fn = train_step_pmap\n",
        "    eval_step_fn = eval_step_pmap\n",
        "    \n",
        "    print(\"✓ Multi-GPU training enabled (pmap)\")\n",
        "else:\n",
        "    # Single device\n",
        "    train_step_fn = train_step\n",
        "    eval_step_fn = eval_step\n",
        "    print(\"Single-GPU training (no pmap)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_batches(X, y, batch_size, shuffle=True, num_devices=1):\n",
        "    \"\"\"Create batches, optionally sharded across devices\"\"\"\n",
        "    n_samples = len(X)\n",
        "    indices = np.arange(n_samples)\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    \n",
        "    # Adjust batch size for multi-device\n",
        "    total_batch_size = batch_size * num_devices\n",
        "    \n",
        "    # Drop last incomplete batch\n",
        "    n_batches = n_samples // total_batch_size\n",
        "    indices = indices[:n_batches * total_batch_size]\n",
        "    \n",
        "    for i in range(0, len(indices), total_batch_size):\n",
        "        batch_idx = indices[i:i + total_batch_size]\n",
        "        batch_x = X[batch_idx]\n",
        "        batch_y = y[batch_idx]\n",
        "        \n",
        "        if num_devices > 1:\n",
        "            # Reshape for pmap: (num_devices, per_device_batch_size, ...)\n",
        "            batch_x = batch_x.reshape(num_devices, -1, *batch_x.shape[1:])\n",
        "            batch_y = batch_y.reshape(num_devices, -1, *batch_y.shape[1:])\n",
        "        \n",
        "        yield jnp.array(batch_x), jnp.array(batch_y)\n",
        "\n",
        "print(\"✓ Data loader defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "This is a simplified training loop. Set `config.NUM_EPOCHS` to a lower value (e.g., 5-10) for quick testing, or increase to 50+ for full training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_mse': [],\n",
        "    'val_mse': [],\n",
        "    'train_nll': [],\n",
        "    'val_nll': [],\n",
        "    'val_r2_temperature': [],\n",
        "    'val_r2_moisture': [],\n",
        "    'val_r2_clouds': [],\n",
        "    'val_r2_overall': [],\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_state = None\n",
        "\n",
        "num_devices = jax.device_count() if config.USE_PMAP else 1\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting Training\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {config.BATCH_SIZE} × {num_devices} devices = {config.BATCH_SIZE * num_devices}\")\n",
        "print(f\"Training samples: {len(X_train_cnn)}\")\n",
        "print(f\"Validation samples: {len(X_val_cnn)}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config.NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Training\n",
        "    train_metrics_epoch = []\n",
        "    for batch_x, batch_y in create_batches(X_train_cnn, y_train_cnn, config.BATCH_SIZE, \n",
        "                                            shuffle=True, num_devices=num_devices):\n",
        "        if config.USE_PMAP:\n",
        "            state, metrics = train_step_fn(state, batch_x, batch_y)\n",
        "            # Extract metrics from first device\n",
        "            metrics = jax.tree_map(lambda x: x[0], metrics)\n",
        "        else:\n",
        "            state, metrics = train_step_fn(state, batch_x, batch_y)\n",
        "        train_metrics_epoch.append(metrics)\n",
        "    \n",
        "    # Average training metrics\n",
        "    train_loss = np.mean([m['loss'] for m in train_metrics_epoch])\n",
        "    train_mse = np.mean([m['mse'] for m in train_metrics_epoch])\n",
        "    train_nll = np.mean([m['nll'] for m in train_metrics_epoch])\n",
        "    \n",
        "    # Validation\n",
        "    val_metrics_epoch = []\n",
        "    val_predictions = []\n",
        "    val_targets = []\n",
        "    \n",
        "    for batch_x, batch_y in create_batches(X_val_cnn, y_val_cnn, config.BATCH_SIZE, \n",
        "                                            shuffle=False, num_devices=num_devices):\n",
        "        if config.USE_PMAP:\n",
        "            mean, log_std, metrics = eval_step_fn(state, batch_x, batch_y)\n",
        "            # Extract from first device\n",
        "            metrics = jax.tree_map(lambda x: x[0], metrics)\n",
        "            mean = mean[0]  # First device\n",
        "            batch_y_cpu = batch_y[0]  # First device\n",
        "        else:\n",
        "            mean, log_std, metrics = eval_step_fn(state, batch_x, batch_y)\n",
        "            batch_y_cpu = batch_y\n",
        "        \n",
        "        val_metrics_epoch.append(metrics)\n",
        "        val_predictions.append(np.array(mean))\n",
        "        val_targets.append(np.array(batch_y_cpu))\n",
        "    \n",
        "    # Average validation metrics\n",
        "    val_loss = np.mean([m['loss'] for m in val_metrics_epoch])\n",
        "    val_mse = np.mean([m['mse'] for m in val_metrics_epoch])\n",
        "    val_nll = np.mean([m['nll'] for m in val_metrics_epoch])\n",
        "    \n",
        "    # Compute per-variable R²\n",
        "    val_predictions_all = np.concatenate(val_predictions, axis=0)\n",
        "    val_targets_all = np.concatenate(val_targets, axis=0)\n",
        "    r2_metrics = compute_per_variable_r2(\n",
        "        val_predictions_all, val_targets_all, config.VAR_GROUPS, \n",
        "        val_predictions_all.shape[-1]\n",
        "    )\n",
        "    \n",
        "    # Update history\n",
        "    history['train_loss'].append(float(train_loss))\n",
        "    history['val_loss'].append(float(val_loss))\n",
        "    history['train_mse'].append(float(train_mse))\n",
        "    history['val_mse'].append(float(val_mse))\n",
        "    history['train_nll'].append(float(train_nll))\n",
        "    history['val_nll'].append(float(val_nll))\n",
        "    history['val_r2_temperature'].append(float(r2_metrics['r2_temperature']))\n",
        "    history['val_r2_moisture'].append(float(r2_metrics['r2_moisture']))\n",
        "    history['val_r2_clouds'].append(float(r2_metrics['r2_clouds']))\n",
        "    history['val_r2_overall'].append(float(r2_metrics['r2_overall']))\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        if config.USE_PMAP:\n",
        "            # Save from first device\n",
        "            best_state = jax.tree_map(lambda x: x[0], state)\n",
        "        else:\n",
        "            best_state = state\n",
        "    \n",
        "    # Periodic checkpoint\n",
        "    if (epoch + 1) % config.SAVE_EVERY == 0:\n",
        "        ckpt_dir = config.MODEL_DIR / f\"checkpoint_epoch_{epoch+1}\"\n",
        "        ckpt_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        if config.USE_PMAP:\n",
        "            state_to_save = jax.tree_map(lambda x: x[0], state)\n",
        "        else:\n",
        "            state_to_save = state\n",
        "        \n",
        "        checkpoints.save_checkpoint(\n",
        "            ckpt_dir=str(ckpt_dir),\n",
        "            target=state_to_save,\n",
        "            step=epoch + 1,\n",
        "            overwrite=True\n",
        "        )\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1:3d}/{config.NUM_EPOCHS} [{epoch_time:5.1f}s] | \"\n",
        "          f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | \"\n",
        "          f\"R² (T/M/C): {r2_metrics['r2_temperature']:.3f}/{r2_metrics['r2_moisture']:.3f}/{r2_metrics['r2_clouds']:.3f}\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training completed in {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
        "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best model with Orbax\n",
        "best_model_dir = config.MODEL_DIR / \"best_model\"\n",
        "best_model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "checkpointer.save(best_model_dir / \"checkpoint\", best_state)\n",
        "\n",
        "# Save config and history\n",
        "np.savez(\n",
        "    config.MODEL_DIR / \"training_results.npz\",\n",
        "    **history,\n",
        "    best_val_loss=best_val_loss,\n",
        "    config_dict={\n",
        "        'model_type': config.MODEL_TYPE,\n",
        "        'cnn_channels': list(config.CNN_CHANNELS),\n",
        "        'kernel_size': config.CNN_KERNEL_SIZE,\n",
        "        'stochastic_output': config.STOCHASTIC_OUTPUT,\n",
        "        'num_params': param_count,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Model saved to {best_model_dir}\")\n",
        "print(f\"✓ Training results saved to {config.MODEL_DIR / 'training_results.npz'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss curves\n",
        "ax = axes[0, 0]\n",
        "ax.plot(history['train_loss'], label='Train', alpha=0.8)\n",
        "ax.plot(history['val_loss'], label='Validation', alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Total Loss')\n",
        "ax.set_title('Training & Validation Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# MSE curves\n",
        "ax = axes[0, 1]\n",
        "ax.plot(history['train_mse'], label='Train MSE', alpha=0.8)\n",
        "ax.plot(history['val_mse'], label='Val MSE', alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('MSE')\n",
        "ax.set_title('Mean Squared Error')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# R² scores by variable group\n",
        "ax = axes[1, 0]\n",
        "ax.plot(history['val_r2_temperature'], label='Temperature', alpha=0.8)\n",
        "ax.plot(history['val_r2_moisture'], label='Moisture', alpha=0.8)\n",
        "ax.plot(history['val_r2_clouds'], label='Clouds', alpha=0.8)\n",
        "ax.plot(history['val_r2_overall'], label='Overall', alpha=0.8, linewidth=2, color='black')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_title('R² by Variable Group')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim([max(-1, min(history['val_r2_overall']) - 0.1), 1.0])\n",
        "\n",
        "# NLL (if stochastic)\n",
        "ax = axes[1, 1]\n",
        "if config.STOCHASTIC_OUTPUT:\n",
        "    ax.plot(history['train_nll'], label='Train NLL', alpha=0.8)\n",
        "    ax.plot(history['val_nll'], label='Val NLL', alpha=0.8)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Negative Log-Likelihood')\n",
        "    ax.set_title('Uncertainty (NLL)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'Stochastic output disabled', \n",
        "            ha='center', va='center', transform=ax.transAxes)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(config.MODEL_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Training curves saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Test Set Evaluation\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "test_predictions = []\n",
        "test_uncertainties = []\n",
        "test_targets_list = []\n",
        "test_metrics = []\n",
        "\n",
        "# Use best_state for evaluation\n",
        "eval_state = best_state\n",
        "\n",
        "for batch_x, batch_y in create_batches(X_test_cnn, y_test_cnn, config.BATCH_SIZE, \n",
        "                                        shuffle=False, num_devices=1):  # Always use 1 device for eval\n",
        "    variables = {'params': eval_state.params}\n",
        "    mean, log_std = model.apply(variables, batch_x, training=False, rngs={'dropout': eval_state.dropout_rng})\n",
        "    \n",
        "    # Compute metrics\n",
        "    loss, (_, _, metrics) = compute_loss(eval_state.params, variables, batch_x, batch_y, \n",
        "                                         eval_state.dropout_rng, training=False)\n",
        "    \n",
        "    test_predictions.append(np.array(mean))\n",
        "    if log_std is not None:\n",
        "        test_uncertainties.append(np.array(jnp.exp(log_std)))\n",
        "    test_targets_list.append(np.array(batch_y))\n",
        "    test_metrics.append(metrics)\n",
        "\n",
        "test_pred = np.concatenate(test_predictions, axis=0)\n",
        "test_targets = np.concatenate(test_targets_list, axis=0)\n",
        "if config.STOCHASTIC_OUTPUT:\n",
        "    test_std = np.concatenate(test_uncertainties, axis=0)\n",
        "\n",
        "# Compute test metrics\n",
        "test_loss = np.mean([m['loss'] for m in test_metrics])\n",
        "test_mse = np.mean([m['mse'] for m in test_metrics])\n",
        "test_mae = np.mean(np.abs(test_pred - test_targets))\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2_metrics = compute_per_variable_r2(test_pred, test_targets, config.VAR_GROUPS, test_pred.shape[-1])\n",
        "\n",
        "print(f\"Test Loss:       {test_loss:.6f}\")\n",
        "print(f\"Test MSE:        {test_mse:.6f}\")\n",
        "print(f\"Test MAE:        {test_mae:.6f}\")\n",
        "print(f\"Test RMSE:       {test_rmse:.6f}\")\n",
        "print(f\"\\nR² Scores:\")\n",
        "print(f\"  Overall:       {test_r2_metrics['r2_overall']:.4f}\")\n",
        "print(f\"  Temperature:   {test_r2_metrics['r2_temperature']:.4f}\")\n",
        "print(f\"  Moisture:      {test_r2_metrics['r2_moisture']:.4f}\")\n",
        "print(f\"  Clouds:        {test_r2_metrics['r2_clouds']:.4f}\")\n",
        "\n",
        "if config.STOCHASTIC_OUTPUT:\n",
        "    mean_uncertainty = np.mean(test_std)\n",
        "    print(f\"\\nUncertainty (mean std): {mean_uncertainty:.6f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Predictions & Uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vertical profile comparison\n",
        "sample_idx = np.random.randint(0, len(test_pred))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# For each output variable, plot a vertical profile\n",
        "for var_idx in range(min(3, n_output_vars)):\n",
        "    ax = axes[var_idx]\n",
        "    \n",
        "    levels = np.arange(n_levels)\n",
        "    pred_profile = test_pred[sample_idx, :, var_idx]\n",
        "    target_profile = test_targets[sample_idx, :, var_idx]\n",
        "    \n",
        "    ax.plot(pred_profile, levels, 'b-', label='Prediction', linewidth=2)\n",
        "    ax.plot(target_profile, levels, 'r--', label='Target', linewidth=2)\n",
        "    \n",
        "    if config.STOCHASTIC_OUTPUT:\n",
        "        std_profile = test_std[sample_idx, :, var_idx]\n",
        "        ax.fill_betweenx(levels, \n",
        "                         pred_profile - 2*std_profile, \n",
        "                         pred_profile + 2*std_profile,\n",
        "                         alpha=0.3, color='blue', label='±2σ uncertainty')\n",
        "    \n",
        "    ax.set_ylabel('Vertical Level')\n",
        "    ax.set_xlabel(f'Variable {var_idx+1}')\n",
        "    ax.set_title(f'Vertical Profile - Output Var {var_idx+1}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.invert_yaxis()  # Top of atmosphere at top\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(config.MODEL_DIR / 'vertical_profiles.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Vertical profiles saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction vs Target Scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: prediction vs target (flattened)\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "pred_flat = test_pred.flatten()\n",
        "target_flat = test_targets.flatten()\n",
        "\n",
        "# Subsample for visualization\n",
        "n_plot = min(10000, len(pred_flat))\n",
        "plot_idx = np.random.choice(len(pred_flat), n_plot, replace=False)\n",
        "\n",
        "ax.scatter(target_flat[plot_idx], pred_flat[plot_idx], alpha=0.3, s=1)\n",
        "ax.plot([target_flat.min(), target_flat.max()], \n",
        "        [target_flat.min(), target_flat.max()], \n",
        "        'r--', linewidth=2, label='Perfect prediction')\n",
        "ax.set_xlabel('Target')\n",
        "ax.set_ylabel('Prediction')\n",
        "ax.set_title(f'Prediction vs Target (R²={test_r2_metrics[\"r2_overall\"]:.3f})')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(config.MODEL_DIR / 'prediction_scatter.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Prediction scatter saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spherical Fourier Neural Operator (SFNO) with torch-harmonics\n",
        "\n",
        "The Spherical FNO is an advanced architecture for global atmospheric modeling using spherical harmonic transforms.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Spherical Harmonics**: Natural basis for global atmospheric data\n",
        "2. **Fourier Modes**: Learn in spectral space, transform back to physical space\n",
        "3. **Multi-scale**: Capture both large-scale (planetary waves) and small-scale (convection) features\n",
        "\n",
        "### References\n",
        "\n",
        "- **FourCastNet** (NVIDIA): https://arxiv.org/abs/2202.11214\n",
        "- **Spherical FNO**: https://arxiv.org/abs/2306.03838\n",
        "- **torch-harmonics**: https://github.com/NVIDIA/torch-harmonics\n",
        "- **ClimateLearn**: https://github.com/aditya-grover/climate-learn\n",
        "\n",
        "### When to Use SFNO\n",
        "\n",
        "- **Global data**: Working with full atmospheric fields (not just regional subsets)\n",
        "- **Large-scale patterns**: Need to capture planetary waves and teleconnections\n",
        "- **Lat/lon grids**: Data is on equiangular or Gaussian grids\n",
        "- **High performance**: Have GPUs and need efficient spectral transforms\n",
        "\n",
        "### Implementation\n",
        "\n",
        "We use NVIDIA's `torch-harmonics` library for efficient spherical harmonic transforms. The library provides:\n",
        "- Fast SHT with cuFFT acceleration\n",
        "- Distributed transforms for model parallelism\n",
        "- DISCO convolutions on the sphere\n",
        "- Spherical attention mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_HARMONICS_AVAILABLE:\n",
        "    # Production SFNO implementation using torch-harmonics\n",
        "    \n",
        "    class SphericalFourierLayer(nn.Module):\n",
        "        \"\"\"Spherical Fourier layer using torch-harmonics SHT\n",
        "        \n",
        "        This layer:\n",
        "        1. Transforms input from physical space to spectral space (SHT)\n",
        "        2. Applies learnable spectral convolution\n",
        "        3. Transforms back to physical space (inverse SHT)\n",
        "        4. Adds skip connection with MLP\n",
        "        \"\"\"\n",
        "        nlat: int\n",
        "        nlon: int\n",
        "        modes_lat: int = 16  # Number of modes in latitude\n",
        "        modes_lon: int = 16  # Number of modes in longitude\n",
        "        hidden_dim: int = 256\n",
        "        grid: str = \"equiangular\"  # or \"legendre-gauss\"\n",
        "        \n",
        "        def setup(self):\n",
        "            # Note: SHT in torch-harmonics works with PyTorch tensors\n",
        "            # For JAX integration, we'll need to use jax.dlpack or convert\n",
        "            # This is a conceptual implementation showing the structure\n",
        "            \n",
        "            # Spectral weights (learnable in spectral space)\n",
        "            self.spectral_weights_real = self.param(\n",
        "                'spectral_weights_real',\n",
        "                nn.initializers.xavier_uniform(),\n",
        "                (self.modes_lat, self.modes_lon, self.hidden_dim, self.hidden_dim)\n",
        "            )\n",
        "            self.spectral_weights_imag = self.param(\n",
        "                'spectral_weights_imag',\n",
        "                nn.initializers.xavier_uniform(),\n",
        "                (self.modes_lat, self.modes_lon, self.hidden_dim, self.hidden_dim)\n",
        "            )\n",
        "            \n",
        "            # Skip connection MLP\n",
        "            self.mlp = nn.Sequential([\n",
        "                nn.Dense(self.hidden_dim),\n",
        "                nn.gelu,\n",
        "                nn.Dense(self.hidden_dim)\n",
        "            ])\n",
        "        \n",
        "        @nn.compact\n",
        "        def __call__(self, x):\n",
        "            \"\"\"\n",
        "            x: (batch, lat, lon, channels) in physical space\n",
        "            returns: (batch, lat, lon, channels) in physical space\n",
        "            \"\"\"\n",
        "            batch, nlat, nlon, channels = x.shape\n",
        "            residual = x\n",
        "            \n",
        "            # NOTE: For actual torch-harmonics integration:\n",
        "            # 1. Convert JAX array to PyTorch tensor\n",
        "            # 2. Apply RealSHT\n",
        "            # 3. Multiply by spectral weights\n",
        "            # 4. Apply InverseRealSHT\n",
        "            # 5. Convert back to JAX array\n",
        "            \n",
        "            # Simplified version using FFT2D as approximation\n",
        "            # (Real SFNO would use proper spherical harmonics)\n",
        "            x_freq = jnp.fft.rfft2(x, axes=(1, 2))\n",
        "            \n",
        "            # Truncate to keep only low-frequency modes\n",
        "            x_freq_truncated = x_freq[:, :self.modes_lat, :self.modes_lon, :]\n",
        "            \n",
        "            # Apply spectral convolution (matrix multiply in spectral space)\n",
        "            # This is where we learn in Fourier domain\n",
        "            spectral_conv = jnp.einsum(\n",
        "                'blmc,lmcd->blmd',\n",
        "                x_freq_truncated,\n",
        "                self.spectral_weights_real + 1j * self.spectral_weights_imag\n",
        "            )\n",
        "            \n",
        "            # Pad back to original size\n",
        "            x_freq_out = jnp.zeros_like(x_freq)\n",
        "            x_freq_out = x_freq_out.at[:, :self.modes_lat, :self.modes_lon, :].set(spectral_conv)\n",
        "            \n",
        "            # Inverse FFT to get back to physical space\n",
        "            x_out = jnp.fft.irfft2(x_freq_out, s=(nlat, nlon), axes=(1, 2))\n",
        "            \n",
        "            # Add skip connection with MLP\n",
        "            x_skip = self.mlp(residual)\n",
        "            \n",
        "            return x_out + x_skip\n",
        "    \n",
        "    \n",
        "    class SFNOClimateEmulator(nn.Module):\n",
        "        \"\"\"Spherical Fourier Neural Operator for Climate Emulation\n",
        "        \n",
        "        Architecture inspired by NVIDIA's FourCastNet and SFNO paper.\n",
        "        Uses spherical harmonic transforms for efficient learning on the sphere.\n",
        "        \"\"\"\n",
        "        nlat: int = 64  # Latitude grid points\n",
        "        nlon: int = 128  # Longitude grid points (typically 2*nlat)\n",
        "        input_channels: int = 5  # Input variables\n",
        "        output_channels: int = 3  # Output tendencies\n",
        "        hidden_dim: int = 256\n",
        "        num_layers: int = 4\n",
        "        modes_lat: int = 32  # Spectral modes (latitude)\n",
        "        modes_lon: int = 32  # Spectral modes (longitude)\n",
        "        dropout_rate: float = 0.1\n",
        "        grid: str = \"equiangular\"\n",
        "        \n",
        "        @nn.compact\n",
        "        def __call__(self, x, training: bool = False):\n",
        "            \"\"\"\n",
        "            x: (batch, nlat, nlon, input_channels) - atmospheric state on sphere\n",
        "            returns: (batch, nlat, nlon, output_channels) - physics tendencies\n",
        "            \"\"\"\n",
        "            \n",
        "            # Lift: embed to higher dimension\n",
        "            x = nn.Dense(self.hidden_dim, name='lift')(x)\n",
        "            x = nn.gelu(x)\n",
        "            \n",
        "            # Spherical Fourier layers\n",
        "            for i in range(self.num_layers):\n",
        "                x = SphericalFourierLayer(\n",
        "                    nlat=self.nlat,\n",
        "                    nlon=self.nlon,\n",
        "                    modes_lat=self.modes_lat,\n",
        "                    modes_lon=self.modes_lon,\n",
        "                    hidden_dim=self.hidden_dim,\n",
        "                    grid=self.grid,\n",
        "                    name=f'sfno_layer_{i}'\n",
        "                )(x)\n",
        "                x = nn.gelu(x)\n",
        "                x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
        "            \n",
        "            # Project: map back to output space\n",
        "            x = nn.Dense(self.output_channels, name='project')(x)\n",
        "            \n",
        "            return x, None  # No stochastic output for now\n",
        "    \n",
        "    \n",
        "    print(\"\\n✓ SFNO architecture defined using torch-harmonics structure\")\n",
        "    print(\"  Note: This uses FFT2D approximation. For production, use actual SHT from torch-harmonics\")\n",
        "    print(\"  See torch-harmonics examples: https://github.com/NVIDIA/torch-harmonics/tree/main/examples\")\n",
        "    \n",
        "else:\n",
        "    # Fallback if torch-harmonics is not available\n",
        "    print(\"\\n⚠ SFNO not available - torch-harmonics not installed\")\n",
        "    print(\"  To enable SFNO, run: pip install torch-harmonics\")\n",
        "    print(\"  Then restart the kernel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SFNO Usage Example\n",
        "\n",
        "To use SFNO instead of 1D CNN, you'll need to:\n",
        "\n",
        "1. **Install torch-harmonics**: `pip install torch-harmonics`\n",
        "2. **Prepare data as 2D grid**: Reshape your data to (batch, lat, lon, channels) format\n",
        "3. **Set model type**: Change `config.MODEL_TYPE = \"sfno\"`\n",
        "4. **Adjust grid size**: Ensure lat/lon dimensions match your data\n",
        "\n",
        "**Note**: For ClimSim NYC subset, the data is columnar (vertical profiles) rather than spatial 2D grids, so **1D CNN is more appropriate**. SFNO is best for global atmospheric data with explicit lat/lon spatial structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Initialize SFNO model (if available and data is on 2D grid)\n",
        "\n",
        "if TORCH_HARMONICS_AVAILABLE and config.MODEL_TYPE == \"sfno\":\n",
        "    print(\"Initializing SFNO model...\")\n",
        "    \n",
        "    # For SFNO, we need 2D spatial data\n",
        "    # Example dimensions for a small test\n",
        "    nlat_sfno = 32\n",
        "    nlon_sfno = 64\n",
        "    \n",
        "    sfno_model = SFNOClimateEmulator(\n",
        "        nlat=nlat_sfno,\n",
        "        nlon=nlon_sfno,\n",
        "        input_channels=n_input_vars,\n",
        "        output_channels=n_output_vars,\n",
        "        hidden_dim=config.SFNO_WIDTH,\n",
        "        num_layers=config.SFNO_LAYERS,\n",
        "        modes_lat=config.SFNO_MODES,\n",
        "        modes_lon=config.SFNO_MODES,\n",
        "        dropout_rate=config.DROPOUT_RATE,\n",
        "        grid=config.grid if hasattr(config, 'grid') else 'equiangular'\n",
        "    )\n",
        "    \n",
        "    # Initialize with dummy 2D spatial data\n",
        "    rng_sfno = random.PRNGKey(43)\n",
        "    sample_2d = jnp.ones((1, nlat_sfno, nlon_sfno, n_input_vars))\n",
        "    sfno_vars = sfno_model.init({'params': rng_sfno, 'dropout': rng_sfno}, sample_2d, training=False)\n",
        "    sfno_params = sfno_vars['params']\n",
        "    \n",
        "    sfno_param_count = sum(x.size for x in jax.tree_util.tree_leaves(sfno_params))\n",
        "    \n",
        "    print(f\"\\nSFNO Model Initialized:\")\n",
        "    print(f\"  Grid: {nlat_sfno}×{nlon_sfno}\")\n",
        "    print(f\"  Parameters: {sfno_param_count:,}\")\n",
        "    print(f\"  Spectral modes: {config.SFNO_MODES}\")\n",
        "    print(f\"  Hidden dim: {config.SFNO_WIDTH}\")\n",
        "    \n",
        "    # Test forward pass\n",
        "    output_2d, _ = sfno_model.apply(sfno_vars, sample_2d, training=False, rngs={'dropout': rng_sfno})\n",
        "    print(f\"  Output shape: {output_2d.shape}\")\n",
        "    \n",
        "    print(\"\\n⚠ Note: To actually train SFNO, you need data in (batch, lat, lon, channels) format\")\n",
        "    print(\"  Current ClimSim data is columnar (vertical profiles), so 1D CNN is more suitable\")\n",
        "    \n",
        "elif config.MODEL_TYPE == \"sfno\" and not TORCH_HARMONICS_AVAILABLE:\n",
        "    print(\"⚠ Cannot initialize SFNO: torch-harmonics not installed\")\n",
        "    print(\"  Install with: pip install torch-harmonics\")\n",
        "else:\n",
        "    print(f\"Using {config.MODEL_TYPE.upper()} architecture (as configured)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JAX-PyTorch Interoperability for torch-harmonics\n",
        "\n",
        "If you want to use actual torch-harmonics SHT in a JAX workflow, you can bridge the frameworks using `jax.dlpack` and `torch.utils.dlpack`. Here's a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_HARMONICS_AVAILABLE:\n",
        "    from jax import dlpack as jax_dlpack\n",
        "    from torch.utils import dlpack as torch_dlpack\n",
        "    \n",
        "    def jax_to_torch(x_jax):\n",
        "        \"\"\"Convert JAX array to PyTorch tensor via DLPack (zero-copy)\"\"\"\n",
        "        x_dlpack = jax_dlpack.to_dlpack(x_jax)\n",
        "        x_torch = torch_dlpack.from_dlpack(x_dlpack)\n",
        "        return x_torch\n",
        "    \n",
        "    def torch_to_jax(x_torch):\n",
        "        \"\"\"Convert PyTorch tensor to JAX array via DLPack (zero-copy)\"\"\"\n",
        "        x_dlpack = torch_dlpack.to_dlpack(x_torch)\n",
        "        x_jax = jax_dlpack.from_dlpack(x_dlpack)\n",
        "        return x_jax\n",
        "    \n",
        "    def apply_torch_sht_in_jax(x_jax, nlat, nlon, grid=\"equiangular\"):\n",
        "        \"\"\"\n",
        "        Apply torch-harmonics SHT within a JAX workflow\n",
        "        \n",
        "        Args:\n",
        "            x_jax: JAX array of shape (batch, nlat, nlon, channels)\n",
        "            nlat: Number of latitude points\n",
        "            nlon: Number of longitude points\n",
        "            grid: \"equiangular\" or \"legendre-gauss\"\n",
        "        \n",
        "        Returns:\n",
        "            Spectral coefficients as JAX array\n",
        "        \"\"\"\n",
        "        # Convert to PyTorch\n",
        "        x_torch = jax_to_torch(x_jax)\n",
        "        \n",
        "        # Move to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        x_torch = x_torch.to(device)\n",
        "        \n",
        "        # Create SHT operator\n",
        "        sht = th.RealSHT(nlat, nlon, grid=grid).to(device)\n",
        "        \n",
        "        # Apply transform (over last 2 dimensions)\n",
        "        # torch-harmonics expects (batch, channels, nlat, nlon)\n",
        "        x_torch = x_torch.permute(0, 3, 1, 2)\n",
        "        coeffs_torch = sht(x_torch)\n",
        "        \n",
        "        # Convert back to JAX\n",
        "        coeffs_jax = torch_to_jax(coeffs_torch)\n",
        "        \n",
        "        return coeffs_jax\n",
        "    \n",
        "    print(\"✓ JAX-PyTorch interop functions defined\")\n",
        "    print(\"  Use these to integrate torch-harmonics SHT in JAX training loops\")\n",
        "    print(\"  Note: This may have performance overhead due to framework conversion\")\n",
        "    \n",
        "    # Example usage\n",
        "    if config.MODEL_TYPE == \"sfno\":\n",
        "        example_2d = jnp.ones((2, 32, 64, 3))  # Small test\n",
        "        print(f\"\\nExample: Applying SHT to shape {example_2d.shape}\")\n",
        "        try:\n",
        "            coeffs_example = apply_torch_sht_in_jax(example_2d, nlat=32, nlon=64)\n",
        "            print(f\"  Spectral coefficients shape: {coeffs_example.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  (Would work with GPU/proper setup, got: {type(e).__name__})\")\n",
        "else:\n",
        "    print(\"JAX-PyTorch interop not available (torch-harmonics not installed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ADVANCED CLIMATE EMULATOR - SUMMARY\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "print(\"✅ What We Built:\\n\")\n",
        "print(\"  1. 1D CNN Architecture\")\n",
        "print(f\"     - {len(config.CNN_CHANNELS)} residual blocks with {config.CNN_CHANNELS} channels\")\n",
        "print(f\"     - Group normalization, dropout, swish activation\")\n",
        "print(f\"     - Parameters: {param_count:,}\\n\")\n",
        "\n",
        "print(\"  2. Stochastic Output Head\")\n",
        "print(f\"     - Predicts mean and uncertainty (std) for each output\")\n",
        "print(f\"     - Enables probabilistic forecasting and risk assessment\\n\")\n",
        "\n",
        "print(\"  3. Per-Variable-Group Monitoring\")\n",
        "print(f\"     - R² tracked separately for temperature, moisture, clouds\")\n",
        "print(f\"     - Final R² scores:\")\n",
        "print(f\"       * Temperature:  {test_r2_metrics['r2_temperature']:.4f}\")\n",
        "print(f\"       * Moisture:     {test_r2_metrics['r2_moisture']:.4f}\")\n",
        "print(f\"       * Clouds:       {test_r2_metrics['r2_clouds']:.4f}\")\n",
        "print(f\"       * Overall:      {test_r2_metrics['r2_overall']:.4f}\\n\")\n",
        "\n",
        "if config.USE_PMAP:\n",
        "    print(\"  4. Multi-GPU Training (pmap)\")\n",
        "    print(f\"     - Data parallel across {jax.device_count()} devices\")\n",
        "    print(f\"     - Effective batch size: {config.BATCH_SIZE * jax.device_count()}\\n\")\n",
        "\n",
        "print(f\"\\n📊 Performance Metrics:\\n\")\n",
        "print(f\"  Test MSE:    {test_mse:.6f}\")\n",
        "print(f\"  Test MAE:    {test_mae:.6f}\")\n",
        "print(f\"  Test RMSE:   {test_rmse:.6f}\")\n",
        "print(f\"  Test R²:     {test_r2_metrics['r2_overall']:.4f}\")\n",
        "if config.STOCHASTIC_OUTPUT:\n",
        "    print(f\"  Mean Std:    {mean_uncertainty:.6f}\")\n",
        "\n",
        "print(f\"\\n💾 Saved Artifacts:\\n\")\n",
        "print(f\"  {config.MODEL_DIR / 'best_model' / 'checkpoint'}\")\n",
        "print(f\"  {config.MODEL_DIR / 'training_results.npz'}\")\n",
        "print(f\"  {config.MODEL_DIR / 'training_curves.png'}\")\n",
        "print(f\"  {config.MODEL_DIR / 'vertical_profiles.png'}\")\n",
        "print(f\"  {config.MODEL_DIR / 'prediction_scatter.png'}\")\n",
        "\n",
        "print(f\"\\n🚀 Next Steps:\\n\")\n",
        "print(\"  1. **Hyperparameter Tuning**\")\n",
        "print(\"     - Try different channel progressions\")\n",
        "print(\"     - Experiment with kernel sizes (3, 5, 7)\")\n",
        "print(\"     - Adjust dropout and weight decay\\n\")\n",
        "\n",
        "print(\"  2. **Advanced Architectures**\")\n",
        "print(\"     - Implement U-Net style skip connections\")\n",
        "print(\"     - Add attention mechanisms\")\n",
        "print(\"     - Explore Spherical FNO for global modeling\\n\")\n",
        "\n",
        "print(\"  3. **Physics Constraints**\")\n",
        "print(\"     - Enable water conservation loss\")\n",
        "print(\"     - Add energy conservation constraints\")\n",
        "print(\"     - Enforce physical bounds (e.g., humidity > 0)\\n\")\n",
        "\n",
        "print(\"  4. **Ensemble Methods**\")\n",
        "print(\"     - Train multiple models with different seeds\")\n",
        "print(\"     - Use stochastic head for ensemble generation\")\n",
        "print(\"     - Calibrate uncertainty estimates\\n\")\n",
        "\n",
        "print(\"  5. **Scaling to Full Dataset**\")\n",
        "print(\"     - Train on full ClimSim low-res (not just NYC)\")\n",
        "print(\"     - Use multi-node training if available\")\n",
        "print(\"     - Implement gradient checkpointing for memory efficiency\\n\")\n",
        "\n",
        "print(\"  6. **Evaluation & Analysis**\")\n",
        "print(\"     - Analyze per-level performance\")\n",
        "print(\"     - Test on extreme events\")\n",
        "print(\"     - Compare with physics-based parameterizations\\n\")\n",
        "\n",
        "print(f\"{'='*80}\")\n",
        "print(\"🌍 Your advanced climate emulator is ready for experimentation! 🚀\")\n",
        "print(f\"{'='*80}\\n\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
