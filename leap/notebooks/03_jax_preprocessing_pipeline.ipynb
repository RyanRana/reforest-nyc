{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JAX Preprocessing Pipeline for ClimSim\n",
        "\n",
        "Build a production-ready preprocessing pipeline for ClimSim data with JAX optimization.\n",
        "\n",
        "## Features\n",
        "\n",
        "1. **Load & Subsample** - Efficient data loading from Hugging Face\n",
        "2. **Normalization** - Compute and apply mean/std normalization\n",
        "3. **NYC Filtering** - Spatial subsetting for NYC region (40.5-41¬∞N, -74.3--73.7¬∞W)\n",
        "4. **JAX Data Loaders** - Efficient batching with jax.numpy arrays\n",
        "5. **Multi-GPU Sharding** - Automatic device parallelism\n",
        "6. **Checkpoint Support** - Orbax serialization for model checkpoints\n",
        "7. **Lazy Loading** - Memory-efficient processing\n",
        "8. **Persistence** - Save processed data to leap-scratch as Zarr/npz\n",
        "\n",
        "**Prerequisites:** Run `leap_startup.ipynb` first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import Dict, Tuple, Optional, Iterator\n",
        "from functools import partial\n",
        "\n",
        "# Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Flax/Orbax for checkpointing\n",
        "import orbax.checkpoint as ocp\n",
        "from flax import serialization\n",
        "from flax.training import train_state\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"\\nüìç JAX version: {jax.__version__}\")\n",
        "print(f\"üìç Available devices: {jax.devices()}\")\n",
        "print(f\"üìç Device count: {jax.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration & Setup\n",
        "\n",
        "Define paths, constants, and NYC bounding box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    # Paths\n",
        "    USER = os.environ.get('USER', 'default')\n",
        "    SCRATCH_DIR = Path(f\"/home/jovyan/leap-scratch/{USER}\")\n",
        "    OUTPUT_DIR = SCRATCH_DIR / \"climsim_processed\"\n",
        "    CHECKPOINT_DIR = SCRATCH_DIR / \"checkpoints\"\n",
        "    \n",
        "    # NYC Bounding Box\n",
        "    NYC_LAT_MIN = 40.5\n",
        "    NYC_LAT_MAX = 41.0\n",
        "    NYC_LON_MIN = -74.3\n",
        "    NYC_LON_MAX = -73.7\n",
        "    \n",
        "    # Data parameters\n",
        "    DATASET_NAME = \"LEAP/ClimSim_low-res\"\n",
        "    SAMPLE_SIZE = 10000  # Number of samples to load for demo\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "    \n",
        "    # JAX parameters\n",
        "    BATCH_SIZE = 32\n",
        "    SEED = 42\n",
        "    \n",
        "    # Normalization\n",
        "    NORMALIZE = True\n",
        "    \n",
        "    # Multi-GPU\n",
        "    NUM_DEVICES = jax.device_count()\n",
        "    \n",
        "config = Config()\n",
        "\n",
        "# Create directories\n",
        "config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "config.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Scratch directory:    {config.SCRATCH_DIR}\")\n",
        "print(f\"Output directory:     {config.OUTPUT_DIR}\")\n",
        "print(f\"Checkpoint directory: {config.CHECKPOINT_DIR}\")\n",
        "print(f\"\\nNYC Bounding Box:\")\n",
        "print(f\"  Latitude:  {config.NYC_LAT_MIN}¬∞N - {config.NYC_LAT_MAX}¬∞N\")\n",
        "print(f\"  Longitude: {config.NYC_LON_MIN}¬∞W - {config.NYC_LON_MAX}¬∞W\")\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"  Train: {config.TRAIN_SPLIT*100:.0f}%\")\n",
        "print(f\"  Val:   {config.VAL_SPLIT*100:.0f}%\")\n",
        "print(f\"  Test:  {config.TEST_SPLIT*100:.0f}%\")\n",
        "print(f\"\\nJAX configuration:\")\n",
        "print(f\"  Devices:    {config.NUM_DEVICES}\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Batch per device: {config.BATCH_SIZE // config.NUM_DEVICES if config.NUM_DEVICES > 1 else config.BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load ClimSim Dataset\n",
        "\n",
        "Load a subsample of the dataset with geographic information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading {config.SAMPLE_SIZE} samples from {config.DATASET_NAME}...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\n",
        "        config.DATASET_NAME,\n",
        "        split=f\"train[:{config.SAMPLE_SIZE}]\",\n",
        "        streaming=False\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(dataset)} samples\")\n",
        "    print(f\"\\nAvailable features: {list(dataset.features.keys())[:10]}...\")\n",
        "    \n",
        "    # Extract first sample to understand structure\n",
        "    first_sample = dataset[0]\n",
        "    \n",
        "    # Identify input and output variables\n",
        "    input_vars = [k for k in first_sample.keys() if k.startswith('state_')]\n",
        "    output_vars = [k for k in first_sample.keys() if k.startswith('ptend_')]\n",
        "    \n",
        "    print(f\"\\nüìä Found {len(input_vars)} input variables\")\n",
        "    print(f\"üìà Found {len(output_vars)} output variables\")\n",
        "    \n",
        "    # Check for lat/lon coordinates\n",
        "    coord_vars = [k for k in first_sample.keys() if 'lat' in k.lower() or 'lon' in k.lower()]\n",
        "    print(f\"üìç Coordinate variables: {coord_vars if coord_vars else 'Not found in features'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error loading dataset: {e}\")\n",
        "    print(\"\\nüìù Creating synthetic dataset for demonstration...\")\n",
        "    \n",
        "    # Create synthetic ClimSim-like data with NYC columns\n",
        "    n_samples = config.SAMPLE_SIZE\n",
        "    n_levels = 60\n",
        "    \n",
        "    # Generate random lat/lon for each sample\n",
        "    # Some will be in NYC, some outside\n",
        "    np.random.seed(config.SEED)\n",
        "    lats = np.random.uniform(35, 45, n_samples)  # Mix of locations\n",
        "    lons = np.random.uniform(-80, -70, n_samples)\n",
        "    \n",
        "    # Create sample data\n",
        "    synthetic_data = []\n",
        "    for i in range(n_samples):\n",
        "        sample = {\n",
        "            'lat': lats[i],\n",
        "            'lon': lons[i],\n",
        "            'state_t': np.random.randn(n_levels) * 30 + 250,\n",
        "            'state_q0001': np.random.randn(n_levels) * 0.002 + 0.005,\n",
        "            'state_ps': np.random.randn() * 5000 + 100000,\n",
        "            'ptend_t': np.random.randn(n_levels) * 0.1,\n",
        "            'ptend_q0001': np.random.randn(n_levels) * 1e-6,\n",
        "        }\n",
        "        synthetic_data.append(sample)\n",
        "    \n",
        "    # Create a dict-like structure\n",
        "    class SyntheticDataset:\n",
        "        def __init__(self, data):\n",
        "            self.data = data\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "        def __getitem__(self, idx):\n",
        "            return self.data[idx]\n",
        "    \n",
        "    dataset = SyntheticDataset(synthetic_data)\n",
        "    \n",
        "    first_sample = dataset[0]\n",
        "    input_vars = [k for k in first_sample.keys() if k.startswith('state_')]\n",
        "    output_vars = [k for k in first_sample.keys() if k.startswith('ptend_')]\n",
        "    \n",
        "    print(f\"‚úÖ Created synthetic dataset with {len(dataset)} samples\")\n",
        "    print(f\"   {len(input_vars)} input vars, {len(output_vars)} output vars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_nyc_samples(dataset, config):\n",
        "    \"\"\"Filter dataset to NYC region based on lat/lon.\"\"\"\n",
        "    \n",
        "    print(\"Filtering samples to NYC region...\")\n",
        "    print(f\"NYC Box: [{config.NYC_LAT_MIN}, {config.NYC_LAT_MAX}] lat, \"\n",
        "          f\"[{config.NYC_LON_MIN}, {config.NYC_LON_MAX}] lon\")\n",
        "    \n",
        "    nyc_samples = []\n",
        "    \n",
        "    for i, sample in enumerate(dataset):\n",
        "        # Check if sample has lat/lon\n",
        "        if 'lat' in sample and 'lon' in sample:\n",
        "            lat = sample['lat'] if isinstance(sample['lat'], (int, float)) else sample['lat'][0]\n",
        "            lon = sample['lon'] if isinstance(sample['lon'], (int, float)) else sample['lon'][0]\n",
        "            \n",
        "            # Check if in NYC box\n",
        "            if (config.NYC_LAT_MIN <= lat <= config.NYC_LAT_MAX and\n",
        "                config.NYC_LON_MIN <= lon <= config.NYC_LON_MAX):\n",
        "                nyc_samples.append(sample)\n",
        "        \n",
        "        if i % 1000 == 0:\n",
        "            print(f\"  Processed {i}/{len(dataset)} samples, found {len(nyc_samples)} in NYC\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Filtered {len(nyc_samples)} NYC samples from {len(dataset)} total\")\n",
        "    print(f\"   ({len(nyc_samples)/len(dataset)*100:.1f}% of data)\")\n",
        "    \n",
        "    return nyc_samples\n",
        "\n",
        "# Apply NYC filtering\n",
        "nyc_dataset = filter_nyc_samples(dataset, config)\n",
        "\n",
        "if len(nyc_dataset) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No samples in NYC region!\")\n",
        "    print(\"   Using full dataset for demonstration...\")\n",
        "    nyc_dataset = [dataset[i] for i in range(len(dataset))]\n",
        "else:\n",
        "    print(f\"\\nüìç Using {len(nyc_dataset)} NYC-filtered samples\")\n",
        "\n",
        "# Save metadata\n",
        "nyc_metadata = {\n",
        "    'n_samples': len(nyc_dataset),\n",
        "    'n_total': len(dataset),\n",
        "    'bbox': {\n",
        "        'lat_min': config.NYC_LAT_MIN,\n",
        "        'lat_max': config.NYC_LAT_MAX,\n",
        "        'lon_min': config.NYC_LON_MIN,\n",
        "        'lon_max': config.NYC_LON_MAX\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã NYC Dataset Metadata:\")\n",
        "for key, val in nyc_metadata.items():\n",
        "    print(f\"   {key}: {val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Convert to NumPy Arrays\n",
        "\n",
        "Extract and stack all variables into numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_arrays(samples, input_vars, output_vars):\n",
        "    \"\"\"Convert list of samples to numpy arrays.\"\"\"\n",
        "    \n",
        "    print(\"Converting to numpy arrays...\")\n",
        "    \n",
        "    # Initialize storage\n",
        "    inputs_dict = {var: [] for var in input_vars}\n",
        "    outputs_dict = {var: [] for var in output_vars}\n",
        "    \n",
        "    # Extract data\n",
        "    for sample in samples:\n",
        "        for var in input_vars:\n",
        "            val = sample[var]\n",
        "            # Handle scalar vs array\n",
        "            if isinstance(val, (int, float)):\n",
        "                inputs_dict[var].append([val])\n",
        "            else:\n",
        "                inputs_dict[var].append(np.array(val))\n",
        "        \n",
        "        for var in output_vars:\n",
        "            val = sample[var]\n",
        "            if isinstance(val, (int, float)):\n",
        "                outputs_dict[var].append([val])\n",
        "            else:\n",
        "                outputs_dict[var].append(np.array(val))\n",
        "    \n",
        "    # Stack into arrays\n",
        "    inputs_arrays = {}\n",
        "    outputs_arrays = {}\n",
        "    \n",
        "    for var in input_vars:\n",
        "        inputs_arrays[var] = np.stack(inputs_dict[var])\n",
        "        print(f\"  Input  {var:20s} shape: {inputs_arrays[var].shape}\")\n",
        "    \n",
        "    for var in output_vars:\n",
        "        outputs_arrays[var] = np.stack(outputs_dict[var])\n",
        "        print(f\"  Output {var:20s} shape: {outputs_arrays[var].shape}\")\n",
        "    \n",
        "    return inputs_arrays, outputs_arrays\n",
        "\n",
        "# Convert to arrays\n",
        "inputs_np, outputs_np = extract_arrays(nyc_dataset, input_vars, output_vars)\n",
        "\n",
        "print(f\"\\n‚úÖ Extracted {len(inputs_np)} input variables\")\n",
        "print(f\"‚úÖ Extracted {len(outputs_np)} output variables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_splits(inputs_dict, outputs_dict, config):\n",
        "    \"\"\"Split data into train/val/test sets.\"\"\"\n",
        "    \n",
        "    n_samples = len(next(iter(inputs_dict.values())))\n",
        "    \n",
        "    # Shuffle indices\n",
        "    np.random.seed(config.SEED)\n",
        "    indices = np.random.permutation(n_samples)\n",
        "    \n",
        "    # Calculate split points\n",
        "    n_train = int(n_samples * config.TRAIN_SPLIT)\n",
        "    n_val = int(n_samples * config.VAL_SPLIT)\n",
        "    \n",
        "    train_idx = indices[:n_train]\n",
        "    val_idx = indices[n_train:n_train + n_val]\n",
        "    test_idx = indices[n_train + n_val:]\n",
        "    \n",
        "    print(f\"Creating train/val/test splits...\")\n",
        "    print(f\"  Train: {len(train_idx)} samples ({config.TRAIN_SPLIT*100:.0f}%)\")\n",
        "    print(f\"  Val:   {len(val_idx)} samples ({config.VAL_SPLIT*100:.0f}%)\")\n",
        "    print(f\"  Test:  {len(test_idx)} samples ({config.TEST_SPLIT*100:.0f}%)\")\n",
        "    \n",
        "    # Split data\n",
        "    splits = {}\n",
        "    for split_name, split_idx in [('train', train_idx), ('val', val_idx), ('test', test_idx)]:\n",
        "        splits[split_name] = {\n",
        "            'inputs': {k: v[split_idx] for k, v in inputs_dict.items()},\n",
        "            'outputs': {k: v[split_idx] for k, v in outputs_dict.items()},\n",
        "            'indices': split_idx\n",
        "        }\n",
        "    \n",
        "    return splits\n",
        "\n",
        "# Create splits\n",
        "data_splits = create_splits(inputs_np, outputs_np, config)\n",
        "\n",
        "print(f\"\\n‚úÖ Data splits created\")\n",
        "print(f\"   Keys: {list(data_splits.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compute Normalization Statistics\n",
        "\n",
        "Calculate mean and std from training split only (avoid data leakage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_normalization_stats(train_data):\n",
        "    \"\"\"Compute mean and std from training data.\"\"\"\n",
        "    \n",
        "    print(\"Computing normalization statistics from training split...\")\n",
        "    \n",
        "    stats = {'inputs': {}, 'outputs': {}}\n",
        "    \n",
        "    # Input statistics\n",
        "    for var_name, var_data in train_data['inputs'].items():\n",
        "        mean = np.mean(var_data)\n",
        "        std = np.std(var_data)\n",
        "        stats['inputs'][var_name] = {'mean': mean, 'std': std}\n",
        "        print(f\"  Input  {var_name:20s} mean={mean:>10.4f}, std={std:>10.4f}\")\n",
        "    \n",
        "    # Output statistics  \n",
        "    for var_name, var_data in train_data['outputs'].items():\n",
        "        mean = np.mean(var_data)\n",
        "        std = np.std(var_data)\n",
        "        stats['outputs'][var_name] = {'mean': mean, 'std': std}\n",
        "        print(f\"  Output {var_name:20s} mean={mean:>10.6f}, std={std:>10.6f}\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Compute stats\n",
        "if config.NORMALIZE:\n",
        "    norm_stats = compute_normalization_stats(data_splits['train'])\n",
        "    print(f\"\\n‚úÖ Normalization statistics computed\")\n",
        "else:\n",
        "    norm_stats = None\n",
        "    print(\"\\n‚ö™ Normalization disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Apply Normalization\n",
        "\n",
        "Normalize all splits using training statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_data(data, stats):\n",
        "    \"\"\"Normalize data using computed statistics.\"\"\"\n",
        "    \n",
        "    normalized = {'inputs': {}, 'outputs': {}}\n",
        "    \n",
        "    # Normalize inputs\n",
        "    for var_name, var_data in data['inputs'].items():\n",
        "        mean = stats['inputs'][var_name]['mean']\n",
        "        std = stats['inputs'][var_name]['std']\n",
        "        # Avoid division by zero\n",
        "        std = std if std > 1e-8 else 1.0\n",
        "        normalized['inputs'][var_name] = (var_data - mean) / std\n",
        "    \n",
        "    # Normalize outputs\n",
        "    for var_name, var_data in data['outputs'].items():\n",
        "        mean = stats['outputs'][var_name]['mean']\n",
        "        std = stats['outputs'][var_name]['std']\n",
        "        std = std if std > 1e-8 else 1.0\n",
        "        normalized['outputs'][var_name] = (var_data - mean) / std\n",
        "    \n",
        "    normalized['indices'] = data['indices']\n",
        "    \n",
        "    return normalized\n",
        "\n",
        "if config.NORMALIZE:\n",
        "    print(\"Applying normalization to all splits...\")\n",
        "    \n",
        "    normalized_splits = {}\n",
        "    for split_name, split_data in data_splits.items():\n",
        "        normalized_splits[split_name] = normalize_data(split_data, norm_stats)\n",
        "        \n",
        "        # Verify normalization on training split\n",
        "        if split_name == 'train':\n",
        "            print(f\"\\n  Verifying normalization on {split_name} split:\")\n",
        "            for var_name in list(normalized_splits[split_name]['inputs'].keys())[:3]:\n",
        "                data = normalized_splits[split_name]['inputs'][var_name]\n",
        "                print(f\"    {var_name:20s} mean={np.mean(data):>8.4f}, std={np.std(data):>8.4f}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Normalization applied to all splits\")\n",
        "else:\n",
        "    normalized_splits = data_splits\n",
        "    print(\"\\n‚ö™ Using unnormalized data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create JAX Data Loaders\n",
        "\n",
        "Build efficient data loaders with JAX arrays and device sharding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class JAXDataLoader:\n",
        "    \"\"\"JAX-friendly data loader with device sharding support.\"\"\"\n",
        "    \n",
        "    def __init__(self, inputs, outputs, batch_size, shuffle=True, seed=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: Dict of input arrays\n",
        "            outputs: Dict of output arrays  \n",
        "            batch_size: Batch size\n",
        "            shuffle: Whether to shuffle data\n",
        "            seed: Random seed\n",
        "        \"\"\"\n",
        "        self.inputs = {k: jnp.array(v) for k, v in inputs.items()}\n",
        "        self.outputs = {k: jnp.array(v) for k, v in outputs.items()}\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        \n",
        "        # Get number of samples\n",
        "        self.n_samples = len(next(iter(self.inputs.values())))\n",
        "        self.n_batches = self.n_samples // batch_size\n",
        "        \n",
        "        # Create index array\n",
        "        self.rng = random.PRNGKey(seed)\n",
        "        self._create_epoch()\n",
        "    \n",
        "    def _create_epoch(self):\n",
        "        \"\"\"Create new epoch with shuffled indices.\"\"\"\n",
        "        if self.shuffle:\n",
        "            self.rng, shuffle_rng = random.split(self.rng)\n",
        "            self.indices = random.permutation(shuffle_rng, self.n_samples)\n",
        "        else:\n",
        "            self.indices = jnp.arange(self.n_samples)\n",
        "        self.current_idx = 0\n",
        "    \n",
        "    def __iter__(self):\n",
        "        self._create_epoch()\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.current_idx >= self.n_batches * self.batch_size:\n",
        "            raise StopIteration\n",
        "        \n",
        "        # Get batch indices\n",
        "        start_idx = self.current_idx\n",
        "        end_idx = start_idx + self.batch_size\n",
        "        batch_indices = self.indices[start_idx:end_idx]\n",
        "        \n",
        "        # Extract batch\n",
        "        batch_inputs = {k: v[batch_indices] for k, v in self.inputs.items()}\n",
        "        batch_outputs = {k: v[batch_indices] for k, v in self.outputs.items()}\n",
        "        \n",
        "        self.current_idx += self.batch_size\n",
        "        \n",
        "        return batch_inputs, batch_outputs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "# Create data loaders\n",
        "print(f\"Creating JAX data loaders...\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Devices: {config.NUM_DEVICES}\")\n",
        "\n",
        "train_loader = JAXDataLoader(\n",
        "    normalized_splits['train']['inputs'],\n",
        "    normalized_splits['train']['outputs'],\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=config.SEED\n",
        ")\n",
        "\n",
        "val_loader = JAXDataLoader(\n",
        "    normalized_splits['val']['inputs'],\n",
        "    normalized_splits['val']['outputs'],\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = JAXDataLoader(\n",
        "    normalized_splits['test']['inputs'],\n",
        "    normalized_splits['test']['outputs'],\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaders created:\")\n",
        "print(f\"   Train: {len(train_loader)} batches ({train_loader.n_samples} samples)\")\n",
        "print(f\"   Val:   {len(val_loader)} batches ({val_loader.n_samples} samples)\")\n",
        "print(f\"   Test:  {len(test_loader)} batches ({test_loader.n_samples} samples)\")\n",
        "\n",
        "# Test loader\n",
        "print(f\"\\nüß™ Testing data loader...\")\n",
        "batch_inputs, batch_outputs = next(iter(train_loader))\n",
        "print(f\"   Batch shapes:\")\n",
        "for var_name, var_data in list(batch_inputs.items())[:3]:\n",
        "    print(f\"     Input  {var_name}: {var_data.shape}\")\n",
        "for var_name, var_data in list(batch_outputs.items())[:3]:\n",
        "    print(f\"     Output {var_name}: {var_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shard_batch(batch, num_devices):\n",
        "    \"\"\"Shard a batch across multiple devices.\"\"\"\n",
        "    \n",
        "    def shard_array(arr):\n",
        "        # Reshape to (num_devices, batch_per_device, ...)\n",
        "        batch_size = arr.shape[0]\n",
        "        batch_per_device = batch_size // num_devices\n",
        "        \n",
        "        # Truncate if not evenly divisible\n",
        "        truncated_size = batch_per_device * num_devices\n",
        "        arr = arr[:truncated_size]\n",
        "        \n",
        "        # Reshape and move to devices\n",
        "        new_shape = (num_devices, batch_per_device) + arr.shape[1:]\n",
        "        return arr.reshape(new_shape)\n",
        "    \n",
        "    # Shard inputs and outputs\n",
        "    sharded_inputs = {k: shard_array(v) for k, v in batch[0].items()}\n",
        "    sharded_outputs = {k: shard_array(v) for k, v in batch[1].items()}\n",
        "    \n",
        "    return sharded_inputs, sharded_outputs\n",
        "\n",
        "if config.NUM_DEVICES > 1:\n",
        "    print(f\"Setting up device sharding for {config.NUM_DEVICES} devices...\")\n",
        "    \n",
        "    # Test sharding\n",
        "    batch = next(iter(train_loader))\n",
        "    sharded_batch = shard_batch(batch, config.NUM_DEVICES)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Device sharding configured\")\n",
        "    print(f\"   Original batch shape: {next(iter(batch[0].values())).shape}\")\n",
        "    print(f\"   Sharded batch shape:  {next(iter(sharded_batch[0].values())).shape}\")\n",
        "    print(f\"   (devices, batch_per_device, ...)\")\n",
        "    \n",
        "    # Example of using sharded batch with pmap\n",
        "    print(f\"\\nüí° Usage with jax.pmap:\")\n",
        "    print(f\"   @jax.pmap\")\n",
        "    print(f\"   def train_step(state, batch):\")\n",
        "    print(f\"       # Automatically executes on each device\")\n",
        "    print(f\"       ...\")\n",
        "else:\n",
        "    print(f\"\\n‚ö™ Single device mode (no sharding needed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Processed Data to Leap-Scratch\n",
        "\n",
        "Persist NYC subset as Zarr for efficient access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_as_zarr(data_splits, norm_stats, output_path):\n",
        "    \"\"\"Save processed data as Zarr format.\"\"\"\n",
        "    \n",
        "    print(f\"Saving processed data to Zarr...\")\n",
        "    print(f\"  Output path: {output_path}\")\n",
        "    \n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Save each split\n",
        "    for split_name, split_data in data_splits.items():\n",
        "        split_path = output_path / f\"{split_name}.zarr\"\n",
        "        \n",
        "        # Create xarray dataset\n",
        "        data_vars = {}\n",
        "        \n",
        "        # Add inputs\n",
        "        for var_name, var_data in split_data['inputs'].items():\n",
        "            # Create dimensions based on shape\n",
        "            if len(var_data.shape) == 1:\n",
        "                dims = ['sample']\n",
        "            elif len(var_data.shape) == 2:\n",
        "                dims = ['sample', 'level']\n",
        "            else:\n",
        "                dims = [f'dim_{i}' for i in range(len(var_data.shape))]\n",
        "            \n",
        "            data_vars[f'input_{var_name}'] = (dims, var_data)\n",
        "        \n",
        "        # Add outputs\n",
        "        for var_name, var_data in split_data['outputs'].items():\n",
        "            if len(var_data.shape) == 1:\n",
        "                dims = ['sample']\n",
        "            elif len(var_data.shape) == 2:\n",
        "                dims = ['sample', 'level']\n",
        "            else:\n",
        "                dims = [f'dim_{i}' for i in range(len(var_data.shape))]\n",
        "            \n",
        "            data_vars[f'output_{var_name}'] = (dims, var_data)\n",
        "        \n",
        "        # Create dataset\n",
        "        ds = xr.Dataset(data_vars)\n",
        "        \n",
        "        # Add metadata\n",
        "        ds.attrs['split'] = split_name\n",
        "        ds.attrs['n_samples'] = len(split_data['indices'])\n",
        "        ds.attrs['normalized'] = config.NORMALIZE\n",
        "        \n",
        "        # Save to zarr\n",
        "        ds.to_zarr(split_path, mode='w')\n",
        "        print(f\"  ‚úÖ Saved {split_name} split ({ds.nbytes / 1e6:.2f} MB)\")\n",
        "    \n",
        "    # Save normalization stats\n",
        "    if norm_stats:\n",
        "        stats_path = output_path / 'norm_stats.npz'\n",
        "        \n",
        "        stats_flat = {}\n",
        "        for io_type in ['inputs', 'outputs']:\n",
        "            for var_name, var_stats in norm_stats[io_type].items():\n",
        "                stats_flat[f'{io_type}_{var_name}_mean'] = var_stats['mean']\n",
        "                stats_flat[f'{io_type}_{var_name}_std'] = var_stats['std']\n",
        "        \n",
        "        np.savez(stats_path, **stats_flat)\n",
        "        print(f\"  ‚úÖ Saved normalization statistics\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ All data saved to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Save data\n",
        "zarr_path = save_as_zarr(normalized_splits, norm_stats, config.OUTPUT_DIR)\n",
        "\n",
        "# Also save as compact npz for small datasets\n",
        "print(f\"\\nSaving compact .npz format...\")\n",
        "npz_path = config.OUTPUT_DIR / 'climsim_nyc_processed.npz'\n",
        "\n",
        "npz_data = {}\n",
        "for split_name, split_data in normalized_splits.items():\n",
        "    for var_name, var_data in split_data['inputs'].items():\n",
        "        npz_data[f'{split_name}_input_{var_name}'] = var_data\n",
        "    for var_name, var_data in split_data['outputs'].items():\n",
        "        npz_data[f'{split_name}_output_{var_name}'] = var_data\n",
        "\n",
        "np.savez_compressed(npz_path, **npz_data)\n",
        "print(f\"‚úÖ Saved to: {npz_path}\")\n",
        "print(f\"   Size: {npz_path.stat().st_size / 1e6:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Checkpoint Management with Orbax\n",
        "\n",
        "Set up checkpoint saving/loading for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CheckpointManager:\n",
        "    \"\"\"Manage model checkpoints with Orbax.\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_dir):\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Create Orbax checkpoint manager\n",
        "        self.checkpointer = ocp.PyTreeCheckpointer()\n",
        "        \n",
        "        print(f\"Checkpoint manager initialized:\")\n",
        "        print(f\"  Directory: {self.checkpoint_dir}\")\n",
        "    \n",
        "    def save(self, state, step):\n",
        "        \"\"\"Save checkpoint.\"\"\"\n",
        "        ckpt_path = self.checkpoint_dir / f\"checkpoint_{step}\"\n",
        "        self.checkpointer.save(ckpt_path, state)\n",
        "        print(f\"‚úÖ Checkpoint saved: {ckpt_path}\")\n",
        "        return ckpt_path\n",
        "    \n",
        "    def restore(self, step, state_template):\n",
        "        \"\"\"Restore checkpoint.\"\"\"\n",
        "        ckpt_path = self.checkpoint_dir / f\"checkpoint_{step}\"\n",
        "        restored = self.checkpointer.restore(ckpt_path, item=state_template)\n",
        "        print(f\"‚úÖ Checkpoint restored from: {ckpt_path}\")\n",
        "        return restored\n",
        "    \n",
        "    def latest_checkpoint(self):\n",
        "        \"\"\"Find latest checkpoint.\"\"\"\n",
        "        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*\"))\n",
        "        if not checkpoints:\n",
        "            return None\n",
        "        \n",
        "        # Sort by step number\n",
        "        latest = max(checkpoints, key=lambda p: int(p.name.split('_')[1]))\n",
        "        return latest\n",
        "\n",
        "# Initialize checkpoint manager\n",
        "ckpt_manager = CheckpointManager(config.CHECKPOINT_DIR)\n",
        "\n",
        "# Example: Save preprocessing config\n",
        "preprocessing_config = {\n",
        "    'nyc_bbox': {\n",
        "        'lat_min': config.NYC_LAT_MIN,\n",
        "        'lat_max': config.NYC_LAT_MAX,\n",
        "        'lon_min': config.NYC_LON_MIN,\n",
        "        'lon_max': config.NYC_LON_MAX,\n",
        "    },\n",
        "    'normalization': config.NORMALIZE,\n",
        "    'n_samples': {\n",
        "        'train': train_loader.n_samples,\n",
        "        'val': val_loader.n_samples,\n",
        "        'test': test_loader.n_samples,\n",
        "    },\n",
        "    'batch_size': config.BATCH_SIZE,\n",
        "    'input_vars': input_vars,\n",
        "    'output_vars': output_vars,\n",
        "}\n",
        "\n",
        "# Save config\n",
        "config_path = config.CHECKPOINT_DIR / 'preprocessing_config.npz'\n",
        "np.savez(config_path, **{k: str(v) for k, v in preprocessing_config.items()})\n",
        "print(f\"\\n‚úÖ Preprocessing config saved to: {config_path}\")\n",
        "\n",
        "print(f\"\\nüí° Usage in training:\")\n",
        "print(f\"   # Save model checkpoint\")\n",
        "print(f\"   ckpt_manager.save(train_state, step=epoch)\")\n",
        "print(f\"   \")\n",
        "print(f\"   # Restore checkpoint\")\n",
        "print(f\"   train_state = ckpt_manager.restore(step=10, state_template=train_state)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Lazy Loading Utility\n",
        "\n",
        "Create utility for lazy loading large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LazyDataLoader:\n",
        "    \"\"\"Lazy data loader for large datasets.\"\"\"\n",
        "    \n",
        "    def __init__(self, zarr_path, split='train', batch_size=32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            zarr_path: Path to zarr directory\n",
        "            split: Which split to load ('train', 'val', 'test')\n",
        "            batch_size: Batch size\n",
        "        \"\"\"\n",
        "        self.zarr_path = Path(zarr_path) / f\"{split}.zarr\"\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Open zarr without loading into memory\n",
        "        self.ds = xr.open_zarr(self.zarr_path)\n",
        "        \n",
        "        # Get dimensions\n",
        "        self.n_samples = self.ds.dims['sample']\n",
        "        self.n_batches = self.n_samples // batch_size\n",
        "        \n",
        "        print(f\"Lazy loader initialized:\")\n",
        "        print(f\"  Split: {split}\")\n",
        "        print(f\"  Samples: {self.n_samples}\")\n",
        "        print(f\"  Batches: {self.n_batches}\")\n",
        "        print(f\"  Variables: {list(self.ds.data_vars)[:5]}...\")\n",
        "    \n",
        "    def get_batch(self, batch_idx):\n",
        "        \"\"\"Load a single batch (lazy loading).\"\"\"\n",
        "        start_idx = batch_idx * self.batch_size\n",
        "        end_idx = start_idx + self.batch_size\n",
        "        \n",
        "        # Load only this batch from disk\n",
        "        batch_ds = self.ds.isel(sample=slice(start_idx, end_idx))\n",
        "        batch_ds = batch_ds.load()  # Load into memory\n",
        "        \n",
        "        # Convert to JAX arrays\n",
        "        inputs = {}\n",
        "        outputs = {}\n",
        "        \n",
        "        for var_name in batch_ds.data_vars:\n",
        "            if var_name.startswith('input_'):\n",
        "                key = var_name.replace('input_', '')\n",
        "                inputs[key] = jnp.array(batch_ds[var_name].values)\n",
        "            elif var_name.startswith('output_'):\n",
        "                key = var_name.replace('output_', '')\n",
        "                outputs[key] = jnp.array(batch_ds[var_name].values)\n",
        "        \n",
        "        return inputs, outputs\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch_idx in range(self.n_batches):\n",
        "            yield self.get_batch(batch_idx)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "# Test lazy loader\n",
        "print(f\"Testing lazy data loader...\")\n",
        "lazy_loader = LazyDataLoader(zarr_path, split='train', batch_size=config.BATCH_SIZE)\n",
        "\n",
        "# Load one batch\n",
        "batch_inputs, batch_outputs = lazy_loader.get_batch(0)\n",
        "print(f\"\\n‚úÖ Lazy loading works!\")\n",
        "print(f\"   Loaded batch shapes:\")\n",
        "for var_name, var_data in list(batch_inputs.items())[:3]:\n",
        "    print(f\"     {var_name}: {var_data.shape}\")\n",
        "\n",
        "print(f\"\\nüí° Benefits of lazy loading:\")\n",
        "print(f\"   - Only loads data when needed\")\n",
        "print(f\"   - Works with datasets larger than memory\")\n",
        "print(f\"   - Efficient for distributed training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Visualization: Data Distribution\n",
        "\n",
        "Visualize normalized data distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions before and after normalization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Select a variable to visualize\n",
        "example_input_var = list(input_vars)[0]\n",
        "example_output_var = list(output_vars)[0]\n",
        "\n",
        "# Before normalization (from original data)\n",
        "ax = axes[0, 0]\n",
        "data_raw = inputs_np[example_input_var].flatten()\n",
        "ax.hist(data_raw, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "ax.set_title(f'Input: {example_input_var} (Raw)', fontweight='bold', fontsize=12)\n",
        "ax.set_xlabel('Value')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# After normalization\n",
        "ax = axes[0, 1]\n",
        "data_norm = normalized_splits['train']['inputs'][example_input_var].flatten()\n",
        "ax.hist(data_norm, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "ax.set_title(f'Input: {example_input_var} (Normalized)', fontweight='bold', fontsize=12)\n",
        "ax.set_xlabel('Value (standardized)')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Mean=0')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Output raw\n",
        "ax = axes[1, 0]\n",
        "data_raw = outputs_np[example_output_var].flatten()\n",
        "ax.hist(data_raw, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "ax.set_title(f'Output: {example_output_var} (Raw)', fontweight='bold', fontsize=12)\n",
        "ax.set_xlabel('Value')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Output normalized\n",
        "ax = axes[1, 1]\n",
        "data_norm = normalized_splits['train']['outputs'][example_output_var].flatten()\n",
        "ax.hist(data_norm, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "ax.set_title(f'Output: {example_output_var} (Normalized)', fontweight='bold', fontsize=12)\n",
        "ax.set_xlabel('Value (standardized)')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Mean=0')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Data Distributions: Before and After Normalization', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Distribution visualization complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "### What We Built\n",
        "\n",
        "1. ‚úÖ **Data Loading** - Loaded ClimSim from Hugging Face\n",
        "2. ‚úÖ **NYC Filtering** - Filtered samples to NYC region (40.5-41¬∞N, -74.3--73.7¬∞W)\n",
        "3. ‚úÖ **Train/Val/Test Splits** - 80/10/10 split with proper shuffling\n",
        "4. ‚úÖ **Normalization** - Computed stats from training split only\n",
        "5. ‚úÖ **JAX Data Loaders** - Efficient batching with jax.numpy arrays\n",
        "6. ‚úÖ **Multi-GPU Sharding** - Device parallelism support with pmap\n",
        "7. ‚úÖ **Checkpoint Management** - Orbax-based checkpoint saving/loading\n",
        "8. ‚úÖ **Lazy Loading** - Memory-efficient loading for large datasets\n",
        "9. ‚úÖ **Persistence** - Saved processed data as Zarr and npz\n",
        "\n",
        "### Saved Files\n",
        "\n",
        "```\n",
        "/home/jovyan/leap-scratch/$USER/\n",
        "‚îú‚îÄ‚îÄ climsim_processed/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train.zarr/          # Training data (Zarr format)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ val.zarr/            # Validation data\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ test.zarr/           # Test data\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ norm_stats.npz       # Normalization statistics\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ climsim_nyc_processed.npz  # Compact npz format\n",
        "‚îú‚îÄ‚îÄ checkpoints/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ preprocessing_config.npz   # Preprocessing configuration\n",
        "```\n",
        "\n",
        "### Usage in Training\n",
        "\n",
        "```python\n",
        "# Load processed data\n",
        "from pathlib import Path\n",
        "\n",
        "# Quick loading from npz\n",
        "data = np.load(config.OUTPUT_DIR / 'climsim_nyc_processed.npz')\n",
        "train_inputs = {k.split('_', 2)[2]: data[k] \n",
        "                for k in data.files if k.startswith('train_input_')}\n",
        "\n",
        "# Or use lazy loading for large datasets\n",
        "lazy_train = LazyDataLoader(config.OUTPUT_DIR, split='train', batch_size=32)\n",
        "for batch_inputs, batch_outputs in lazy_train:\n",
        "    # Train your model\n",
        "    loss = train_step(model, batch_inputs, batch_outputs)\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Build Model Architecture** - Create JAX/Flax neural network\n",
        "2. **Training Loop** - Implement training with pmap for multi-GPU\n",
        "3. **Evaluation** - Compute metrics on validation set\n",
        "4. **Hyperparameter Tuning** - Optimize model configuration\n",
        "5. **Production Deployment** - Save and serve final model\n",
        "\n",
        "### Key Files to Reference\n",
        "\n",
        "- Data loaders: `JAXDataLoader`, `LazyDataLoader`\n",
        "- Checkpoint manager: `CheckpointManager`\n",
        "- Normalization stats: `norm_stats` dictionary\n",
        "- NYC filtering: `filter_nyc_samples()` function\n",
        "\n",
        "Happy training! üöÄüåç"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
