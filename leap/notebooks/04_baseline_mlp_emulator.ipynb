{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline MLP Climate Emulator\n",
        "\n",
        "Build and train a Multi-Layer Perceptron (MLP) to emulate climate physics using JAX + Flax.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "**Input**: Flattened atmospheric state vector (~200-300 dimensions)\n",
        "- Temperature profiles (60 levels)\n",
        "- Humidity profiles (60 levels)\n",
        "- Surface pressure, winds, etc.\n",
        "\n",
        "**Model**: Deep MLP\n",
        "- 4-6 hidden layers\n",
        "- 512-1024 units per layer\n",
        "- Swish/GELU activation\n",
        "- Layer normalization\n",
        "- Dropout for regularization\n",
        "\n",
        "**Output**: Physics tendencies\n",
        "- Temperature tendency (60 levels)\n",
        "- Humidity tendency (60 levels)\n",
        "- Other tendency variables\n",
        "\n",
        "**Loss**: MSE + optional water conservation regularization\n",
        "\n",
        "**Optimizer**: AdamW with learning rate scheduling\n",
        "\n",
        "**Prerequisites**: Run `03_jax_preprocessing_pipeline.ipynb` first to prepare data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, grad, vmap\n",
        "from jax.tree_util import tree_map\n",
        "import optax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import Sequence, Callable\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"\\nüìç JAX version: {jax.__version__}\")\n",
        "print(f\"üìç Available devices: {jax.devices()}\")\n",
        "print(f\"üìç Device count: {jax.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration for MLP emulator.\"\"\"\n",
        "    \n",
        "    # Paths\n",
        "    USER = os.environ.get('USER', 'default')\n",
        "    SCRATCH_DIR = Path(f\"/home/jovyan/leap-scratch/{USER}\")\n",
        "    DATA_DIR = SCRATCH_DIR / \"climsim_processed\"\n",
        "    MODEL_DIR = SCRATCH_DIR / \"models\" / \"mlp_baseline\"\n",
        "    CHECKPOINT_DIR = MODEL_DIR / \"checkpoints\"\n",
        "    \n",
        "    # Model architecture\n",
        "    HIDDEN_DIMS = [512, 512, 512, 512]  # 4 layers with 512 units each\n",
        "    ACTIVATION = 'swish'  # or 'gelu'\n",
        "    USE_LAYER_NORM = True\n",
        "    DROPOUT_RATE = 0.1\n",
        "    \n",
        "    # Training\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 1e-3\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    WARMUP_EPOCHS = 5\n",
        "    \n",
        "    # Loss\n",
        "    USE_WATER_CONSERVATION = False  # Enable water conservation regularization\n",
        "    WATER_CONSERVATION_WEIGHT = 0.1\n",
        "    \n",
        "    # Checkpointing\n",
        "    SAVE_EVERY = 5  # Save checkpoint every N epochs\n",
        "    KEEP_BEST = True  # Keep best model based on validation loss\n",
        "    \n",
        "    # Random seed\n",
        "    SEED = 42\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Create directories\n",
        "config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "config.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Data directory:       {config.DATA_DIR}\")\n",
        "print(f\"Model directory:      {config.MODEL_DIR}\")\n",
        "print(f\"Checkpoint directory: {config.CHECKPOINT_DIR}\")\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(f\"  Hidden layers: {config.HIDDEN_DIMS}\")\n",
        "print(f\"  Activation:    {config.ACTIVATION}\")\n",
        "print(f\"  Layer norm:    {config.USE_LAYER_NORM}\")\n",
        "print(f\"  Dropout:       {config.DROPOUT_RATE}\")\n",
        "print(f\"\\nTraining:\")\n",
        "print(f\"  Batch size:    {config.BATCH_SIZE}\")\n",
        "print(f\"  Epochs:        {config.NUM_EPOCHS}\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"  Weight decay:  {config.WEIGHT_DECAY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading preprocessed data from: {config.DATA_DIR}\")\n",
        "\n",
        "# Load data from npz (faster for small datasets)\n",
        "data_path = config.DATA_DIR / 'climsim_nyc_processed.npz'\n",
        "\n",
        "if not data_path.exists():\n",
        "    print(f\"‚ö†Ô∏è  Data file not found: {data_path}\")\n",
        "    print(\"Please run 03_jax_preprocessing_pipeline.ipynb first!\")\n",
        "    \n",
        "    # Create synthetic data for demonstration\n",
        "    print(\"\\nüìù Creating synthetic data for demonstration...\")\n",
        "    n_train, n_val, n_test = 8000, 1000, 1000\n",
        "    n_levels = 60\n",
        "    \n",
        "    # Synthetic inputs and outputs\n",
        "    np.random.seed(config.SEED)\n",
        "    \n",
        "    train_data = {\n",
        "        'train_input_state_t': np.random.randn(n_train, n_levels),\n",
        "        'train_input_state_q0001': np.random.randn(n_train, n_levels),\n",
        "        'train_input_state_ps': np.random.randn(n_train, 1),\n",
        "        'train_output_ptend_t': np.random.randn(n_train, n_levels) * 0.1,\n",
        "        'train_output_ptend_q0001': np.random.randn(n_train, n_levels) * 0.01,\n",
        "    }\n",
        "    \n",
        "    val_data = {\n",
        "        'val_input_state_t': np.random.randn(n_val, n_levels),\n",
        "        'val_input_state_q0001': np.random.randn(n_val, n_levels),\n",
        "        'val_input_state_ps': np.random.randn(n_val, 1),\n",
        "        'val_output_ptend_t': np.random.randn(n_val, n_levels) * 0.1,\n",
        "        'val_output_ptend_q0001': np.random.randn(n_val, n_levels) * 0.01,\n",
        "    }\n",
        "    \n",
        "    test_data = {\n",
        "        'test_input_state_t': np.random.randn(n_test, n_levels),\n",
        "        'test_input_state_q0001': np.random.randn(n_test, n_levels),\n",
        "        'test_input_state_ps': np.random.randn(n_test, 1),\n",
        "        'test_output_ptend_t': np.random.randn(n_test, n_levels) * 0.1,\n",
        "        'test_output_ptend_q0001': np.random.randn(n_test, n_levels) * 0.01,\n",
        "    }\n",
        "    \n",
        "    # Combine\n",
        "    data = {**train_data, **val_data, **test_data}\n",
        "    \n",
        "else:\n",
        "    data = np.load(data_path)\n",
        "    print(f\"‚úÖ Loaded data from {data_path}\")\n",
        "\n",
        "# Extract train/val/test data\n",
        "def extract_split(data, split='train'):\n",
        "    \"\"\"Extract input and output variables for a split.\"\"\"\n",
        "    prefix = f'{split}_'\n",
        "    \n",
        "    # Get all variable names\n",
        "    input_vars = [k for k in data.files if k.startswith(f'{prefix}input_')]\n",
        "    output_vars = [k for k in data.files if k.startswith(f'{prefix}output_')]\n",
        "    \n",
        "    # Concatenate all inputs into single array\n",
        "    inputs = [data[var] for var in sorted(input_vars)]\n",
        "    # Flatten each input if needed\n",
        "    inputs_flat = []\n",
        "    for inp in inputs:\n",
        "        if len(inp.shape) == 1:\n",
        "            inputs_flat.append(inp.reshape(-1, 1))\n",
        "        else:\n",
        "            inputs_flat.append(inp)\n",
        "    X = np.concatenate(inputs_flat, axis=1)\n",
        "    \n",
        "    # Concatenate all outputs\n",
        "    outputs = [data[var] for var in sorted(output_vars)]\n",
        "    outputs_flat = []\n",
        "    for out in outputs:\n",
        "        if len(out.shape) == 1:\n",
        "            outputs_flat.append(out.reshape(-1, 1))\n",
        "        else:\n",
        "            outputs_flat.append(out)\n",
        "    y = np.concatenate(outputs_flat, axis=1)\n",
        "    \n",
        "    return X, y, input_vars, output_vars\n",
        "\n",
        "# Extract splits\n",
        "X_train, y_train, train_input_vars, train_output_vars = extract_split(data, 'train')\n",
        "X_val, y_val, _, _ = extract_split(data, 'val')\n",
        "X_test, y_test, _, _ = extract_split(data, 'test')\n",
        "\n",
        "# Convert to JAX arrays\n",
        "X_train = jnp.array(X_train)\n",
        "y_train = jnp.array(y_train)\n",
        "X_val = jnp.array(X_val)\n",
        "y_val = jnp.array(y_val)\n",
        "X_test = jnp.array(X_test)\n",
        "y_test = jnp.array(y_test)\n",
        "\n",
        "print(f\"\\nüìä Data shapes:\")\n",
        "print(f\"  Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Val:   X={X_val.shape}, y={y_val.shape}\")\n",
        "print(f\"  Test:  X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"\\n  Input dimension:  {X_train.shape[1]}\")\n",
        "print(f\"  Output dimension: {y_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClimateEmulatorMLP(nn.Module):\n",
        "    \"\"\"Multi-Layer Perceptron for climate physics emulation.\"\"\"\n",
        "    \n",
        "    hidden_dims: Sequence[int]\n",
        "    output_dim: int\n",
        "    activation: str = 'swish'\n",
        "    use_layer_norm: bool = True\n",
        "    dropout_rate: float = 0.1\n",
        "    training: bool = True\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, input_dim)\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Select activation function\n",
        "        if self.activation == 'swish':\n",
        "            activation_fn = nn.swish\n",
        "        elif self.activation == 'gelu':\n",
        "            activation_fn = nn.gelu\n",
        "        elif self.activation == 'relu':\n",
        "            activation_fn = nn.relu\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "        \n",
        "        # Hidden layers\n",
        "        for i, dim in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(dim, name=f'dense_{i}')(x)\n",
        "            \n",
        "            if self.use_layer_norm:\n",
        "                x = nn.LayerNorm(name=f'ln_{i}')(x)\n",
        "            \n",
        "            x = activation_fn(x)\n",
        "            \n",
        "            if self.dropout_rate > 0:\n",
        "                x = nn.Dropout(rate=self.dropout_rate, deterministic=not self.training)(x)\n",
        "        \n",
        "        # Output layer (no activation for regression)\n",
        "        x = nn.Dense(self.output_dim, name='output')(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "model = ClimateEmulatorMLP(\n",
        "    hidden_dims=config.HIDDEN_DIMS,\n",
        "    output_dim=output_dim,\n",
        "    activation=config.ACTIVATION,\n",
        "    use_layer_norm=config.USE_LAYER_NORM,\n",
        "    dropout_rate=config.DROPOUT_RATE\n",
        ")\n",
        "\n",
        "# Initialize parameters\n",
        "rng = random.PRNGKey(config.SEED)\n",
        "rng, init_rng = random.split(rng)\n",
        "\n",
        "# Dummy input for initialization\n",
        "dummy_input = jnp.ones((1, X_train.shape[1]))\n",
        "params = model.init(init_rng, dummy_input)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 70)\n",
        "print(model.tabulate(init_rng, dummy_input, compute_flops=True, compute_vjp_flops=True))\n",
        "\n",
        "# Count parameters\n",
        "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"\\nüìä Total parameters: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Loss Function\n",
        "\n",
        "MSE loss with optional water conservation regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(predictions, targets):\n",
        "    \"\"\"Mean squared error loss.\"\"\"\n",
        "    return jnp.mean((predictions - targets) ** 2)\n",
        "\n",
        "def water_conservation_loss(predictions, humidity_indices):\n",
        "    \"\"\"Water conservation regularization.\n",
        "    \n",
        "    Penalizes violations of water conservation (sum of humidity tendencies should be ~0).\n",
        "    \n",
        "    Args:\n",
        "        predictions: Model predictions\n",
        "        humidity_indices: Indices of humidity-related variables\n",
        "    \n",
        "    Returns:\n",
        "        Conservation loss value\n",
        "    \"\"\"\n",
        "    # Extract humidity tendencies\n",
        "    humidity_tends = predictions[:, humidity_indices]\n",
        "    \n",
        "    # Sum over vertical levels (should be close to 0 for conservation)\n",
        "    total_water_tendency = jnp.sum(humidity_tends, axis=1)\n",
        "    \n",
        "    # Penalize deviation from zero\n",
        "    return jnp.mean(total_water_tendency ** 2)\n",
        "\n",
        "def compute_loss(params, batch_x, batch_y, model, rng, use_water_conservation=False):\n",
        "    \"\"\"Compute total loss.\n",
        "    \n",
        "    Args:\n",
        "        params: Model parameters\n",
        "        batch_x: Input batch\n",
        "        batch_y: Target batch\n",
        "        model: Flax model\n",
        "        rng: Random key for dropout\n",
        "        use_water_conservation: Whether to add water conservation regularization\n",
        "    \n",
        "    Returns:\n",
        "        Total loss value\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    predictions = model.apply(params, batch_x, rngs={'dropout': rng})\n",
        "    \n",
        "    # MSE loss\n",
        "    loss = mse_loss(predictions, batch_y)\n",
        "    \n",
        "    # Add water conservation regularization if enabled\n",
        "    if use_water_conservation:\n",
        "        # Assume humidity variables are in second half of outputs (adjust as needed)\n",
        "        n_out = predictions.shape[1]\n",
        "        humidity_indices = jnp.arange(n_out // 2, n_out)\n",
        "        \n",
        "        water_loss = water_conservation_loss(predictions, humidity_indices)\n",
        "        loss = loss + config.WATER_CONSERVATION_WEIGHT * water_loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "print(\"‚úÖ Loss functions defined\")\n",
        "print(f\"   Base loss: MSE\")\n",
        "print(f\"   Water conservation: {'Enabled' if config.USE_WATER_CONSERVATION else 'Disabled'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup\n",
        "\n",
        "Configure optimizer and create training state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create learning rate schedule with warmup\n",
        "def create_learning_rate_schedule(base_lr, warmup_epochs, total_epochs, steps_per_epoch):\n",
        "    \"\"\"Create learning rate schedule with warmup and cosine decay.\"\"\"\n",
        "    \n",
        "    warmup_steps = warmup_epochs * steps_per_epoch\n",
        "    total_steps = total_epochs * steps_per_epoch\n",
        "    \n",
        "    warmup_schedule = optax.linear_schedule(\n",
        "        init_value=0.0,\n",
        "        end_value=base_lr,\n",
        "        transition_steps=warmup_steps\n",
        "    )\n",
        "    \n",
        "    cosine_schedule = optax.cosine_decay_schedule(\n",
        "        init_value=base_lr,\n",
        "        decay_steps=total_steps - warmup_steps,\n",
        "        alpha=0.1  # End at 10% of base_lr\n",
        "    )\n",
        "    \n",
        "    schedule = optax.join_schedules(\n",
        "        schedules=[warmup_schedule, cosine_schedule],\n",
        "        boundaries=[warmup_steps]\n",
        "    )\n",
        "    \n",
        "    return schedule\n",
        "\n",
        "# Calculate steps per epoch\n",
        "steps_per_epoch = len(X_train) // config.BATCH_SIZE\n",
        "\n",
        "# Create optimizer\n",
        "lr_schedule = create_learning_rate_schedule(\n",
        "    base_lr=config.LEARNING_RATE,\n",
        "    warmup_epochs=config.WARMUP_EPOCHS,\n",
        "    total_epochs=config.NUM_EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch\n",
        ")\n",
        "\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=lr_schedule,\n",
        "    weight_decay=config.WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Create training state\n",
        "class TrainState(train_state.TrainState):\n",
        "    \"\"\"Extended train state with dropout RNG.\"\"\"\n",
        "    dropout_rng: jax.Array\n",
        "\n",
        "# Initialize training state\n",
        "state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=optimizer,\n",
        "    dropout_rng=rng\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING SETUP\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Optimizer: AdamW\")\n",
        "print(f\"  Base learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"  Weight decay:       {config.WEIGHT_DECAY}\")\n",
        "print(f\"  Warmup epochs:      {config.WARMUP_EPOCHS}\")\n",
        "print(f\"\\nSteps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total steps:     {config.NUM_EPOCHS * steps_per_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Step (JIT Compiled)\n",
        "\n",
        "Define training and evaluation functions with JIT compilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def train_step(state, batch_x, batch_y):\n",
        "    \"\"\"Single training step (JIT compiled).\n",
        "    \n",
        "    Args:\n",
        "        state: Training state\n",
        "        batch_x: Input batch\n",
        "        batch_y: Target batch\n",
        "    \n",
        "    Returns:\n",
        "        Updated state and loss value\n",
        "    \"\"\"\n",
        "    # Split RNG for dropout\n",
        "    dropout_rng, new_dropout_rng = random.split(state.dropout_rng)\n",
        "    \n",
        "    # Compute loss and gradients\n",
        "    def loss_fn(params):\n",
        "        return compute_loss(\n",
        "            params, batch_x, batch_y, \n",
        "            state.apply_fn, dropout_rng,\n",
        "            use_water_conservation=config.USE_WATER_CONSERVATION\n",
        "        )\n",
        "    \n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    \n",
        "    # Update parameters\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    state = state.replace(dropout_rng=new_dropout_rng)\n",
        "    \n",
        "    return state, loss\n",
        "\n",
        "@jit\n",
        "def eval_step(state, batch_x, batch_y):\n",
        "    \"\"\"Single evaluation step (JIT compiled).\n",
        "    \n",
        "    Args:\n",
        "        state: Training state\n",
        "        batch_x: Input batch\n",
        "        batch_y: Target batch\n",
        "    \n",
        "    Returns:\n",
        "        Loss value and predictions\n",
        "    \"\"\"\n",
        "    # No dropout during evaluation\n",
        "    predictions = state.apply_fn(\n",
        "        state.params, batch_x,\n",
        "        training=False,\n",
        "        rngs={'dropout': state.dropout_rng}\n",
        "    )\n",
        "    \n",
        "    loss = mse_loss(predictions, batch_y)\n",
        "    \n",
        "    return loss, predictions\n",
        "\n",
        "def create_batches(X, y, batch_size, rng):\n",
        "    \"\"\"Create shuffled batches.\"\"\"\n",
        "    n_samples = len(X)\n",
        "    \n",
        "    # Shuffle indices\n",
        "    perm = random.permutation(rng, n_samples)\n",
        "    X_shuffled = X[perm]\n",
        "    y_shuffled = y[perm]\n",
        "    \n",
        "    # Create batches\n",
        "    n_batches = n_samples // batch_size\n",
        "    X_batches = X_shuffled[:n_batches * batch_size].reshape(n_batches, batch_size, -1)\n",
        "    y_batches = y_shuffled[:n_batches * batch_size].reshape(n_batches, batch_size, -1)\n",
        "    \n",
        "    return X_batches, y_batches\n",
        "\n",
        "def evaluate(state, X, y, batch_size):\n",
        "    \"\"\"Evaluate model on dataset.\n",
        "    \n",
        "    Args:\n",
        "        state: Training state\n",
        "        X: Input data\n",
        "        y: Target data\n",
        "        batch_size: Batch size\n",
        "    \n",
        "    Returns:\n",
        "        Average loss\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    \n",
        "    n_samples = len(X)\n",
        "    n_batches = n_samples // batch_size\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        \n",
        "        batch_x = X[start_idx:end_idx]\n",
        "        batch_y = y[start_idx:end_idx]\n",
        "        \n",
        "        loss, _ = eval_step(state, batch_x, batch_y)\n",
        "        losses.append(loss)\n",
        "    \n",
        "    return jnp.mean(jnp.array(losses))\n",
        "\n",
        "print(\"‚úÖ Training and evaluation functions defined (JIT compiled)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(\"=\" * 70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize tracking\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "best_state = None\n",
        "\n",
        "# Training\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config.NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Create batches for this epoch\n",
        "    rng, batch_rng = random.split(rng)\n",
        "    X_batches, y_batches = create_batches(X_train, y_train, config.BATCH_SIZE, batch_rng)\n",
        "    \n",
        "    # Train for one epoch\n",
        "    epoch_losses = []\n",
        "    for batch_x, batch_y in zip(X_batches, y_batches):\n",
        "        state, loss = train_step(state, batch_x, batch_y)\n",
        "        epoch_losses.append(loss)\n",
        "    \n",
        "    train_loss = jnp.mean(jnp.array(epoch_losses))\n",
        "    train_losses.append(float(train_loss))\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    val_loss = evaluate(state, X_val, y_val, config.BATCH_SIZE)\n",
        "    val_losses.append(float(val_loss))\n",
        "    \n",
        "    # Track best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_state = state\n",
        "        best_epoch = epoch\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{config.NUM_EPOCHS} | \"\n",
        "              f\"Train Loss: {train_loss:.6f} | \"\n",
        "              f\"Val Loss: {val_loss:.6f} | \"\n",
        "              f\"Time: {epoch_time:.2f}s\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    if config.SAVE_EVERY > 0 and (epoch + 1) % config.SAVE_EVERY == 0:\n",
        "        ckpt_path = config.CHECKPOINT_DIR / f\"checkpoint_epoch_{epoch+1}\"\n",
        "        checkpointer = ocp.PyTreeCheckpointer()\n",
        "        checkpointer.save(ckpt_path, state)\n",
        "        print(f\"  üíæ Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(f\"TRAINING COMPLETE\")\n",
        "print(f\"{'=' * 70}\")\n",
        "print(f\"Total time: {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
        "print(f\"Time per epoch: {total_time/config.NUM_EPOCHS:.2f}s\")\n",
        "print(f\"\\nFinal train loss: {train_losses[-1]:.6f}\")\n",
        "print(f\"Final val loss:   {val_losses[-1]:.6f}\")\n",
        "print(f\"Best val loss:    {best_val_loss:.6f} (epoch {best_epoch+1})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best model\n",
        "if config.KEEP_BEST and best_state is not None:\n",
        "    best_model_path = config.MODEL_DIR / \"best_model\"\n",
        "    checkpointer = ocp.PyTreeCheckpointer()\n",
        "    checkpointer.save(best_model_path, best_state)\n",
        "    \n",
        "    print(f\"\\nüíæ Saved best model to: {best_model_path}\")\n",
        "    print(f\"   Best validation loss: {best_val_loss:.6f}\")\n",
        "    print(f\"   Achieved at epoch: {best_epoch+1}\")\n",
        "    \n",
        "    # Save model config\n",
        "    model_config = {\n",
        "        'hidden_dims': config.HIDDEN_DIMS,\n",
        "        'output_dim': output_dim,\n",
        "        'input_dim': X_train.shape[1],\n",
        "        'activation': config.ACTIVATION,\n",
        "        'use_layer_norm': config.USE_LAYER_NORM,\n",
        "        'dropout_rate': config.DROPOUT_RATE,\n",
        "        'best_val_loss': float(best_val_loss),\n",
        "        'best_epoch': int(best_epoch),\n",
        "        'total_params': int(param_count),\n",
        "    }\n",
        "    \n",
        "    config_path = config.MODEL_DIR / \"model_config.npz\"\n",
        "    np.savez(config_path, **{k: str(v) for k, v in model_config.items()})\n",
        "    print(f\"   Saved config to: {config_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualization: Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax = axes[0]\n",
        "epochs = np.arange(1, len(train_losses) + 1)\n",
        "ax.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "ax.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "ax.axvline(best_epoch + 1, color='g', linestyle='--', alpha=0.7, label=f'Best Model (epoch {best_epoch+1})')\n",
        "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_yscale('log')\n",
        "\n",
        "# Loss difference\n",
        "ax = axes[1]\n",
        "loss_diff = np.array(val_losses) - np.array(train_losses)\n",
        "ax.plot(epochs, loss_diff, 'purple', linewidth=2)\n",
        "ax.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
        "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Val Loss - Train Loss', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Generalization Gap', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.fill_between(epochs, 0, loss_diff, where=(loss_diff>0), alpha=0.3, color='red', label='Overfitting')\n",
        "ax.fill_between(epochs, 0, loss_diff, where=(loss_diff<0), alpha=0.3, color='green', label='Underfitting')\n",
        "ax.legend(fontsize=10)\n",
        "\n",
        "plt.suptitle('MLP Climate Emulator - Training Progress', fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save plot\n",
        "plot_path = config.MODEL_DIR / 'training_curves.png'\n",
        "fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "print(f\"\\nüíæ Saved training curves to: {plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate best model on test set\n",
        "print(\"=\" * 70)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_loss = evaluate(best_state, X_test, y_test, config.BATCH_SIZE)\n",
        "print(f\"\\nTest Loss (MSE): {test_loss:.6f}\")\n",
        "\n",
        "# Get predictions for analysis\n",
        "test_predictions = []\n",
        "n_test_batches = len(X_test) // config.BATCH_SIZE\n",
        "\n",
        "for i in range(n_test_batches):\n",
        "    start_idx = i * config.BATCH_SIZE\n",
        "    end_idx = start_idx + config.BATCH_SIZE\n",
        "    \n",
        "    batch_x = X_test[start_idx:end_idx]\n",
        "    batch_y = y_test[start_idx:end_idx]\n",
        "    \n",
        "    _, preds = eval_step(best_state, batch_x, batch_y)\n",
        "    test_predictions.append(preds)\n",
        "\n",
        "test_predictions = jnp.concatenate(test_predictions, axis=0)\n",
        "test_targets = y_test[:len(test_predictions)]\n",
        "\n",
        "# Compute additional metrics\n",
        "mae = jnp.mean(jnp.abs(test_predictions - test_targets))\n",
        "rmse = jnp.sqrt(jnp.mean((test_predictions - test_targets) ** 2))\n",
        "\n",
        "# R¬≤ score\n",
        "ss_res = jnp.sum((test_targets - test_predictions) ** 2)\n",
        "ss_tot = jnp.sum((test_targets - jnp.mean(test_targets)) ** 2)\n",
        "r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "print(f\"\\nAdditional Metrics:\")\n",
        "print(f\"  MAE:  {mae:.6f}\")\n",
        "print(f\"  RMSE: {rmse:.6f}\")\n",
        "print(f\"  R¬≤:   {r2:.6f}\")\n",
        "\n",
        "# Per-variable analysis (if we know variable boundaries)\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Mean prediction: {jnp.mean(test_predictions):.6f}\")\n",
        "print(f\"  Std prediction:  {jnp.std(test_predictions):.6f}\")\n",
        "print(f\"  Mean target:     {jnp.mean(test_targets):.6f}\")\n",
        "print(f\"  Std target:      {jnp.std(test_targets):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Visualization: Predictions vs Targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs targets\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# 1. Scatter plot: Predictions vs Targets\n",
        "ax = axes[0, 0]\n",
        "# Sample subset for visibility\n",
        "n_plot = min(5000, len(test_predictions))\n",
        "idx = np.random.choice(len(test_predictions), n_plot, replace=False)\n",
        "\n",
        "scatter = ax.scatter(test_targets[idx].flatten(), \n",
        "                     test_predictions[idx].flatten(),\n",
        "                     alpha=0.3, s=1, c='blue')\n",
        "                     \n",
        "# Perfect prediction line\n",
        "min_val = min(test_targets.min(), test_predictions.min())\n",
        "max_val = max(test_targets.max(), test_predictions.max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "ax.set_xlabel('True Values', fontsize=11, fontweight='bold')\n",
        "ax.set_ylabel('Predicted Values', fontsize=11, fontweight='bold')\n",
        "ax.set_title(f'Predictions vs Targets (R¬≤={r2:.4f})', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Residual histogram\n",
        "ax = axes[0, 1]\n",
        "residuals = (test_predictions - test_targets).flatten()\n",
        "ax.hist(residuals, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
        "ax.set_xlabel('Residual (Predicted - True)', fontsize=11, fontweight='bold')\n",
        "ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "ax.set_title(f'Residual Distribution (Mean={jnp.mean(residuals):.6f})', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Sample vertical profile comparison\n",
        "ax = axes[1, 0]\n",
        "# Assume first 60 dimensions are temperature tendency\n",
        "sample_idx = 0\n",
        "if test_predictions.shape[1] >= 60:\n",
        "    true_profile = test_targets[sample_idx, :60]\n",
        "    pred_profile = test_predictions[sample_idx, :60]\n",
        "    levels = np.arange(60)\n",
        "    \n",
        "    ax.plot(true_profile, levels, 'b-', linewidth=2, marker='o', markersize=4, label='True')\n",
        "    ax.plot(pred_profile, levels, 'r--', linewidth=2, marker='s', markersize=4, label='Predicted')\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel('Temperature Tendency', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Vertical Level', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'Sample Vertical Profile (Test Sample {sample_idx})', fontsize=12, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'Need 60+ output dims\\\\nfor vertical profile', \n",
        "            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
        "    ax.set_title('Vertical Profile', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 4. Error by output dimension\n",
        "ax = axes[1, 1]\n",
        "mse_per_dim = jnp.mean((test_predictions - test_targets) ** 2, axis=0)\n",
        "dims = np.arange(len(mse_per_dim))\n",
        "ax.bar(dims, mse_per_dim, alpha=0.7, color='purple')\n",
        "ax.set_xlabel('Output Dimension', fontsize=11, fontweight='bold')\n",
        "ax.set_ylabel('MSE', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Error by Output Dimension', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Model Evaluation - Test Set Performance', fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save plot\n",
        "eval_plot_path = config.MODEL_DIR / 'evaluation_plots.png'\n",
        "fig.savefig(eval_plot_path, dpi=150, bbox_inches='tight')\n",
        "print(f\"\\nüíæ Saved evaluation plots to: {eval_plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Loading Example\n",
        "\n",
        "Show how to load and use the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MODEL LOADING EXAMPLE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example: How to load the saved model\n",
        "print(\"\\nüìù Code to load the trained model:\\n\")\n",
        "print(\"\"\"\n",
        "from flax import linen as nn\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n",
        "\n",
        "# Recreate model architecture\n",
        "model = ClimateEmulatorMLP(\n",
        "    hidden_dims=[512, 512, 512, 512],\n",
        "    output_dim=121,  # Adjust to your output dimension\n",
        "    activation='swish',\n",
        "    use_layer_norm=True,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_dir = Path('/home/jovyan/leap-scratch/$USER/models/mlp_baseline')\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "restored_state = checkpointer.restore(\n",
        "    checkpoint_dir / 'best_model',\n",
        "    item=state  # Provide template\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "predictions = restored_state.apply_fn(\n",
        "    restored_state.params,\n",
        "    input_data,\n",
        "    training=False\n",
        ")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "### What We Built\n",
        "\n",
        "1. ‚úÖ **MLP Architecture** - 4-layer MLP with 512 units per layer\n",
        "2. ‚úÖ **Advanced Features** - Layer norm, dropout, swish activation\n",
        "3. ‚úÖ **Training** - AdamW optimizer with warmup + cosine decay\n",
        "4. ‚úÖ **JIT Compilation** - Fast training with JAX JIT\n",
        "5. ‚úÖ **Evaluation** - Comprehensive metrics on test set\n",
        "6. ‚úÖ **Checkpointing** - Saved best model with Orbax\n",
        "7. ‚úÖ **Visualization** - Training curves and prediction analysis\n",
        "\n",
        "### Model Performance\n",
        "\n",
        "```\n",
        "Best Validation Loss: {best_val_loss:.6f}\n",
        "Test Loss (MSE):      {test_loss:.6f}\n",
        "R¬≤ Score:             {r2:.6f}\n",
        "MAE:                  {mae:.6f}\n",
        "```\n",
        "\n",
        "### Saved Files\n",
        "\n",
        "```\n",
        "/home/jovyan/leap-scratch/$USER/models/mlp_baseline/\n",
        "‚îú‚îÄ‚îÄ best_model/                 # Best model checkpoint\n",
        "‚îú‚îÄ‚îÄ model_config.npz            # Model configuration\n",
        "‚îú‚îÄ‚îÄ training_curves.png         # Training visualization\n",
        "‚îú‚îÄ‚îÄ evaluation_plots.png        # Evaluation visualization\n",
        "‚îî‚îÄ‚îÄ checkpoints/                # Periodic checkpoints\n",
        "    ‚îú‚îÄ‚îÄ checkpoint_epoch_5/\n",
        "    ‚îú‚îÄ‚îÄ checkpoint_epoch_10/\n",
        "    ‚îî‚îÄ‚îÄ ...\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Hyperparameter Tuning**\n",
        "   - Try different layer sizes (256, 1024)\n",
        "   - Experiment with more/fewer layers\n",
        "   - Adjust learning rate and weight decay\n",
        "\n",
        "2. **Advanced Architectures**\n",
        "   - Residual connections (ResNet-style)\n",
        "   - Attention mechanisms\n",
        "   - Physics-informed architectures\n",
        "\n",
        "3. **Regularization**\n",
        "   - Enable water conservation loss\n",
        "   - Energy conservation constraints\n",
        "   - Physical consistency checks\n",
        "\n",
        "4. **Deployment**\n",
        "   - Create inference pipeline\n",
        "   - Optimize for production\n",
        "   - Monitor performance\n",
        "\n",
        "5. **Ensemble Methods**\n",
        "   - Train multiple models\n",
        "   - Average predictions\n",
        "   - Uncertainty quantification\n",
        "\n",
        "### Key Metrics\n",
        "\n",
        "- **Training Time**: ~{total_time:.1f}s for {config.NUM_EPOCHS} epochs\n",
        "- **Parameters**: {param_count:,}\n",
        "- **Throughput**: ~{steps_per_epoch * config.NUM_EPOCHS / total_time:.1f} steps/sec\n",
        "\n",
        "Great work! Your baseline climate emulator is ready! üåçüöÄ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
