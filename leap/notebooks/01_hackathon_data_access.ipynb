{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC LEAP Hackathon 2026 - Data Access Guide\n",
        "\n",
        "Complete guide to accessing hackathon datasets from OSN and Hugging Face.\n",
        "\n",
        "## Available Data Sources\n",
        "\n",
        "1. **OSN Cloud Storage** (https://nyu1.osn.mghpcc.org)\n",
        "   - HRRR (High-Resolution Rapid Refresh)\n",
        "   - ERA5 NYC subset\n",
        "   - CorrDiff model outputs\n",
        "\n",
        "2. **Hugging Face Datasets**\n",
        "   - ClimSim low-res\n",
        "   - ClimSim low-res-expanded\n",
        "\n",
        "**Prerequisites:** Run `leap_startup.ipynb` first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import s3fs\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download, list_repo_files\n",
        "\n",
        "print(\"âœ… All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: OSN Cloud Storage Access\n",
        "\n",
        "### 1.1 Configure Anonymous S3 Filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OSN Configuration for NYC LEAP Hackathon 2026\n",
        "OSN_ENDPOINT_URL = \"https://nyu1.osn.mghpcc.org\"\n",
        "OSN_BUCKET = \"leap-pangeo-manual\"\n",
        "HACKATHON_PREFIX = \"hackathon-2026\"\n",
        "\n",
        "# Create anonymous S3 filesystem (no credentials needed)\n",
        "fs = s3fs.S3FileSystem(\n",
        "    anon=True,\n",
        "    client_kwargs={'endpoint_url': OSN_ENDPOINT_URL}\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"OSN CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"ğŸ“ Endpoint:  {OSN_ENDPOINT_URL}\")\n",
        "print(f\"ğŸ“ Bucket:    {OSN_BUCKET}\")\n",
        "print(f\"ğŸ“ Prefix:    {HACKATHON_PREFIX}\")\n",
        "print(f\"ğŸ“ Full path: s3://{OSN_BUCKET}/{HACKATHON_PREFIX}/\")\n",
        "print(\"\\nâœ… Anonymous S3 filesystem created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 List Top-Level Directories\n",
        "\n",
        "Expected directories: `hrrr/`, `era5_cds/nyc/`, `corrdiff/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List top-level directories under hackathon-2026/\n",
        "base_path = f\"{OSN_BUCKET}/{HACKATHON_PREFIX}/\"\n",
        "\n",
        "print(f\"Listing contents of: s3://{base_path}\\n\")\n",
        "print(\"Available datasets:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    directories = fs.ls(base_path)\n",
        "    \n",
        "    for i, dir_path in enumerate(directories, 1):\n",
        "        dir_name = dir_path.split('/')[-1]\n",
        "        print(f\"{i}. {dir_name}/\")\n",
        "    \n",
        "    print(f\"\\nâœ… Found {len(directories)} top-level directories\")\n",
        "    print(\"\\nExpected directories:\")\n",
        "    print(\"  - hrrr/          (High-Resolution Rapid Refresh data)\")\n",
        "    print(\"  - era5_cds/nyc/  (ERA5 reanalysis for NYC region)\")\n",
        "    print(\"  - corrdiff/      (CorrDiff model outputs)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Open Zarr Dataset Lazily with xarray\n",
        "\n",
        "**Lazy loading** means data isn't loaded into memory until you compute or access it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Open a Zarr store from HRRR or ERA5 lazily\n",
        "print(\"Code template for lazy loading:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\"\"\n",
        "# Open Zarr store with lazy loading\n",
        "zarr_path = f\"s3://{OSN_BUCKET}/{HACKATHON_PREFIX}/hrrr/your-file.zarr\"\n",
        "mapper = fs.get_mapper(zarr_path)\n",
        "ds = xr.open_zarr(mapper, consolidated=True)\n",
        "\n",
        "# Data is NOT loaded into memory yet!\n",
        "print(ds)  # Shows structure but doesn't load data\n",
        "\n",
        "# Access a variable (still lazy)\n",
        "temperature = ds['temperature']\n",
        "\n",
        "# Compute a subset (NOW data is loaded)\n",
        "nyc_subset = ds.sel(lat=slice(40.5, 41.0), lon=slice(-74.5, -73.5))\n",
        "mean_temp = nyc_subset['temperature'].mean().compute()\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Hugging Face ClimSim Dataset\n",
        "\n",
        "### 2.1 ClimSim Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CLIMSIM DATASETS ON HUGGING FACE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "datasets_info = [\n",
        "    {\n",
        "        'name': 'LEAP/ClimSim_low-res',\n",
        "        'description': 'Low-resolution climate simulation data'\n",
        "    },\n",
        "    {\n",
        "        'name': 'LEAP/ClimSim_low-res-expanded',\n",
        "        'description': 'Expanded low-resolution with more variables'\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, dataset in enumerate(datasets_info, 1):\n",
        "    print(f\"\\n{i}. {dataset['name']}\")\n",
        "    print(f\"   {dataset['description']}\")\n",
        "    print(f\"   URL: https://huggingface.co/datasets/{dataset['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Download Small ClimSim Subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download small sample from ClimSim\n",
        "import os\n",
        "\n",
        "# Create download directory in scratch space\n",
        "scratch_dir = Path(f\"/home/jovyan/leap-scratch/{os.environ.get('USER', 'default')}\")\n",
        "download_dir = scratch_dir / \"climsim_samples\"\n",
        "download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Download directory: {download_dir}\\n\")\n",
        "\n",
        "repo_id = \"LEAP/ClimSim_low-res\"\n",
        "\n",
        "try:\n",
        "    # Download README first\n",
        "    print(f\"Downloading README from {repo_id}...\")\n",
        "    \n",
        "    downloaded_path = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=\"README.md\",\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=download_dir,\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… Downloaded to: {downloaded_path}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Load ClimSim with Streamingg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ClimSim with streaming (doesn't download everything)\n",
        "repo_id = \"LEAP/ClimSim_low-res\"\n",
        "\n",
        "print(f\"Loading {repo_id} with streaming...\\n\")\n",
        "print(\"Code template:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Streaming mode - access data without full download\n",
        "dataset = load_dataset(\n",
        "    'LEAP/ClimSim_low-res',\n",
        "    split='train',\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Get first few examples\n",
        "for i, example in enumerate(dataset):\n",
        "    print(f\"Example {i}:\", example.keys())\n",
        "    if i >= 4:\n",
        "        break\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nğŸ“ See the next notebook (02_climsim_analysis.ipynb) for detailed ClimSim inspection!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
